{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c37a1e2d555f4c66b6bb98b915997163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c767706f2c7c44fd9dc6a2cb30e39ab5",
              "IPY_MODEL_c4a0ed5a4e734539ae1e5719e40127dd",
              "IPY_MODEL_1228e08b9da14d0592e31e2beea69fe1"
            ],
            "layout": "IPY_MODEL_eff81c41138442d5b8a1fe1a459aab65"
          }
        },
        "c767706f2c7c44fd9dc6a2cb30e39ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad0b80988321468691dcfe033656a46f",
            "placeholder": "​",
            "style": "IPY_MODEL_dab2be26cdf3461eaf4e0204897b21a5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c4a0ed5a4e734539ae1e5719e40127dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585c01f74d254465adba0bdf06cf8975",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d00cc2b3385492d9dfc0db1d2a70aa2",
            "value": 48
          }
        },
        "1228e08b9da14d0592e31e2beea69fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb01fdba49a4e2ab114d01339b1ae87",
            "placeholder": "​",
            "style": "IPY_MODEL_0dd44e4936254796a4a56ffa0b1d27d6",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.89kB/s]"
          }
        },
        "eff81c41138442d5b8a1fe1a459aab65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad0b80988321468691dcfe033656a46f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dab2be26cdf3461eaf4e0204897b21a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "585c01f74d254465adba0bdf06cf8975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d00cc2b3385492d9dfc0db1d2a70aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fb01fdba49a4e2ab114d01339b1ae87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd44e4936254796a4a56ffa0b1d27d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6081b1d550fa4c5f9c0f868baf35b10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f998201ee4f4867865a2043a99e1c96",
              "IPY_MODEL_c05d165c46514fecb97612929098f440",
              "IPY_MODEL_530704db6e054fd0a6edbe259f0a34b9"
            ],
            "layout": "IPY_MODEL_7d4f4bd8ddbd43f7ad261bd0d1bcd338"
          }
        },
        "2f998201ee4f4867865a2043a99e1c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b17a5f30d14f4463bebc9d6c11093ff7",
            "placeholder": "​",
            "style": "IPY_MODEL_ef9ce6cf40ab4afd91c360636fac1b15",
            "value": "config.json: 100%"
          }
        },
        "c05d165c46514fecb97612929098f440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5f87c9f58f643ddbfee8519be5a364b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb717d2f95b44b7397c38287e9cb6b07",
            "value": 570
          }
        },
        "530704db6e054fd0a6edbe259f0a34b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8ea04b46784f579e0fcd33ec5d1049",
            "placeholder": "​",
            "style": "IPY_MODEL_d6ecb91c9efc4d3e897d410e0760e461",
            "value": " 570/570 [00:00&lt;00:00, 38.7kB/s]"
          }
        },
        "7d4f4bd8ddbd43f7ad261bd0d1bcd338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b17a5f30d14f4463bebc9d6c11093ff7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef9ce6cf40ab4afd91c360636fac1b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5f87c9f58f643ddbfee8519be5a364b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb717d2f95b44b7397c38287e9cb6b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6c8ea04b46784f579e0fcd33ec5d1049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6ecb91c9efc4d3e897d410e0760e461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2b68e8ac0dd4a7db56380103cd39740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_851883a209294d818260494dbf33de23",
              "IPY_MODEL_bc7ab94cffd14f688998f931b08dc546",
              "IPY_MODEL_61a700dd9d7f4505b23bda98327a470f"
            ],
            "layout": "IPY_MODEL_f34fcba5e37c49c892b784f7d0759aba"
          }
        },
        "851883a209294d818260494dbf33de23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e41a8643b064151b23a36e6653869ed",
            "placeholder": "​",
            "style": "IPY_MODEL_31a0972c88c84339a9bec5204d22d33a",
            "value": "vocab.txt: 100%"
          }
        },
        "bc7ab94cffd14f688998f931b08dc546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_894c83321515408ab9bbbc3b02d7c4ed",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7c0eed6cc654b70ab954bd340ee54c9",
            "value": 231508
          }
        },
        "61a700dd9d7f4505b23bda98327a470f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a858c31057447e9b3b5449592d5966",
            "placeholder": "​",
            "style": "IPY_MODEL_1bb4200415be42ec9d6667c5d7708318",
            "value": " 232k/232k [00:00&lt;00:00, 6.72MB/s]"
          }
        },
        "f34fcba5e37c49c892b784f7d0759aba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e41a8643b064151b23a36e6653869ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a0972c88c84339a9bec5204d22d33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "894c83321515408ab9bbbc3b02d7c4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7c0eed6cc654b70ab954bd340ee54c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6a858c31057447e9b3b5449592d5966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb4200415be42ec9d6667c5d7708318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76bc4cffc1214da9b4fa5d1a9edc9afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffb6d4be56704104abdad19efae5ea03",
              "IPY_MODEL_84253beaf10542d19a5fee14ae6153ab",
              "IPY_MODEL_2b67d60c30e24d1b95ae71c7d27e1adb"
            ],
            "layout": "IPY_MODEL_4fb18661f6ce4217864192e8d9e4cc27"
          }
        },
        "ffb6d4be56704104abdad19efae5ea03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd51c94467a449508a4fc376217d5044",
            "placeholder": "​",
            "style": "IPY_MODEL_d4ca17ea8f704607945f6a4fc3deea11",
            "value": "tokenizer.json: 100%"
          }
        },
        "84253beaf10542d19a5fee14ae6153ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_128eae96e61e4a0fbace6dd382e02bcd",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0fa12309009494685d022f277a2445e",
            "value": 466062
          }
        },
        "2b67d60c30e24d1b95ae71c7d27e1adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_761c6390531d4bb0ad2e0c53e36cd643",
            "placeholder": "​",
            "style": "IPY_MODEL_94806d7d738b410eb7d8ca2d8785a0db",
            "value": " 466k/466k [00:00&lt;00:00, 19.6MB/s]"
          }
        },
        "4fb18661f6ce4217864192e8d9e4cc27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd51c94467a449508a4fc376217d5044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ca17ea8f704607945f6a4fc3deea11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "128eae96e61e4a0fbace6dd382e02bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0fa12309009494685d022f277a2445e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "761c6390531d4bb0ad2e0c53e36cd643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94806d7d738b410eb7d8ca2d8785a0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutharimanikanta/-technity-tasks-/blob/main/Language_Translation_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "#!pip install --upgrade transformers"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bR5cSCCYnomq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6hvhc-1hv8SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk5r5gUem72l"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# # Load translation pipeline\n",
        "# translator = pipeline(\"translation\", model=\"Telugu-LLM-Labs/gemma_2b_hi_finetuned\", tokenizer=\"Telugu-LLM-Labs/gemma_2b_hi_finetuned\")\n",
        "\n",
        "# # Example Telugu sentence\n",
        "# telugu_text = \"నేను ఊహించలేను.\"\n",
        "\n",
        "# # Translate Telugu to English\n",
        "# english_translation = translator(telugu_text, src_lang=\"te\", tgt_lang=\"en\")\n",
        "\n",
        "# print(english_translation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "GVBE-nKbYX_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Download nltk resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def _init_(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self)._init_()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        embeddings = self.embedding_layer(inputs)\n",
        "        output, state = self.gru(embeddings)\n",
        "        return output, state\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def _init_(self, units=128):\n",
        "        super(BahdanauAttention, self)._init_()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # Expand dimension for broadcasting addition operation\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # Calculate the attention scores\n",
        "        scores = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
        "\n",
        "        # Compute context vector\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def _init_(self, vocab_size=1000, embedding_size=128, units=128):\n",
        "        super(Decoder, self)._init_()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # Used for attention\n",
        "        self.attention = BahdanauAttention(units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # Passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "class AdditiveAttentionTranslator:\n",
        "    encoder_input_words = 20\n",
        "    vocab_size = 1000\n",
        "    embedding_size = 128\n",
        "    epochs = 10\n",
        "    batch_size = 200\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
        "    loss_history = []\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "\n",
        "        encode = encode = Encoder(vocab_size = self.vocab_size, embedding_size = self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        x_decoder = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size = self.embedding_size, vocab_size = self.vocab_size, words = self.encoder_input_words)((x_decoder[:,0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs = decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def generate_random_data(self, instances = 1000, decoder_words = 10):\n",
        "        X1, X2 = np.random.randint(self.vocab_size, size=(instances, self.encoder_input_words)), np.random.randint(self.vocab_size, size=(instances, decoder_words))\n",
        "        Y = Y = np.eye(self.vocab_size)[np.random.choice(self.vocab_size, instances * decoder_words)].reshape(instances, decoder_words, self.vocab_size)\n",
        "        self.X1, self.X2, self.Y = X1, X2, Y\n",
        "        return X1, X2, Y\n",
        "\n",
        "    def train_translator(self):\n",
        "        tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "        optimizer, loss_fn = self.optimizer, self.loss_fn\n",
        "\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = tf.shape(self.Y)[0]\n",
        "\n",
        "        X1, X2, Y = self.X1, self.X2, self.Y\n",
        "\n",
        "        self.get_enc_dec()\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X1[batch : batch + batch_size]\n",
        "                    x2_train = X2[batch : batch + batch_size]\n",
        "                    y_train = Y[batch : batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(y_train[:, query_number], output)\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, keys, query_start, query_size = None):\n",
        "        if query_size == None:\n",
        "            query_size = self.X2.shape[-1]\n",
        "        H, state = self.encoder(keys)\n",
        "\n",
        "        value = []\n",
        "        state_steps = []\n",
        "        value.append(int(query_start[0][0]))\n",
        "\n",
        "\n",
        "        for query_number in range(query_size):\n",
        "            output, state = self.decoder((query_start, state, H))\n",
        "            query_start = np.argmax(output.numpy(), axis = -1)\n",
        "            value.append(query_start[0])\n",
        "            state_steps.append(state)\n",
        "\n",
        "        return value, state_steps\n",
        "    def preprocess_input_sentence(self, input_sentence):\n",
        "        # Tokenize the input sentence using nltk word_tokenize\n",
        "        tokens = word_tokenize(input_sentence)\n",
        "\n",
        "        # Limit the number of tokens to the maximum input length\n",
        "        max_input_length = self.encoder_input_words\n",
        "        tokens = tokens[:max_input_length]\n",
        "\n",
        "        # Convert tokens to indices using a pretrained tokenizer\n",
        "        input_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        return input_indices\n",
        "\n",
        "    def postprocess_output(self, translated_indices):\n",
        "        # Convert translated indices to tokens using the same tokenizer used for preprocessing\n",
        "        translated_tokens = self.tokenizer.convert_ids_to_tokens(translated_indices)\n",
        "\n",
        "        # Join tokens into a sentence\n",
        "        translated_sentence = ' '.join(translated_tokens)\n",
        "\n",
        "        return translated_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN9euEplPubD",
        "outputId": "c7dda259-e754-4f46-a4c5-a6971cb90246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A Gated Recurrent Unit (GRU) layer is a type of recurrent neural network (RNN) layer used for processing sequential data, such as text, time series, and audio. It is designed to address some of the limitations of traditional RNNs, particularly regarding the vanishing gradient problem and the difficulty in capturing long-range dependencies in sequences.\n",
        "\n",
        "* i want use Transformer Encoder Layers"
      ],
      "metadata": {
        "id": "qfdsVedChyLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Bi-directional RNNs:\n",
        "Purpose: Bi-directional RNNs process sequences in both forward and backward directions.\n",
        "Strengths:\n",
        "Capture context from both past and future tokens.\n",
        "Useful for tasks like part-of-speech tagging, named entity recognition, and sentiment analysis.\n",
        "Handle sequential dependencies well.\n",
        "Weaknesses:\n",
        "Computationally expensive due to bidirectional processing.\n",
        "Still suffer from vanishing gradient problems.\n",
        "Example: Bidirectional LSTMs or GRUs.\n",
        "* Transformer Networks:\n",
        "Purpose: Transformers revolutionized NLP by introducing self-attention mechanisms.\n",
        "Strengths:\n",
        "Parallelizable, making them faster than RNNs.\n",
        "Capture global context effectively.\n",
        "State-of-the-art performance on various NLP benchmarks.\n",
        "Weaknesses:\n",
        "Require large amounts of data and computational resources.\n",
        "Lack inherent sequential processing (no recurrence).\n",
        "Example: BERT, GPT, and other transformer-based models.\n",
        "When to Choose:\n",
        "\n",
        "Bi-directional RNNs: Use when you need fine-grained sequential context and have limited data.\n",
        "Transformers: Opt for transformers when you have abundant data, want to handle long-range dependencies, and aim for top-tier performance."
      ],
      "metadata": {
        "id": "8ANsFfZ-q7e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):# here we have passed the Layers so that we can coustomize it\n",
        "  def __init__(self,vocab_size=1000,embedding_size=128):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.vocab_size=vocab_size# indicate top most word underconsideration\n",
        "    self.embedding_size=embedding_size #dim of word embeddings\n",
        "  def build(self,input_shape):\n",
        "    self.embedding_layer=tf.keras.layers.Embedding(self.vocab_size,self.embedding_size)\n",
        "    self.gru=tf.keras.layers.GRU(self.embedding_size,return_sequence=True,return_state=True)\n",
        "    self.bi=tf.keras.layers.Bidirectional(self.gru)\n",
        "    #hidden layer or output of gru(embedding_size),GRU layer will output a sequence of vectors instead of just a single vector(,return_sequence),\n",
        "    #the GRU layer will return both the sequence of outputs and the final hidden state.\n",
        "    print()\n",
        "  def call(self,inputs):\n",
        "    words=inputs\n",
        "    embeddings=self.embedding_layer(words)\n",
        "    output_sequence, forward_state, backward_state = self.bi(embeddings)\n",
        "    return (output_sequence, forward_state, backward_state)\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,words=20,embedding_size=128):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.words=words\n",
        "    self.embedding_size=embedding_size\n",
        "  def build(self,input_shapes):\n",
        "    # w2 will hold the trainable weights which we add next to the our coustom layers\n",
        "    # shape for weighted matrix\n",
        "    #initial values of the weight matrix W2 will be randomly sampled from a uniform distribution Using a random initialization is common practice in deep learning\n",
        "    # to prevent the weights from being stuck in a symmetric or zero-initialized state.\n",
        "    self.W1=self.add_weight(shape=(1,self.embedding_size),initializer=\"random_uniform\")\n",
        "    self.W2 = self.add_weight(shape = (self.words, self.embedding_size), initializer = \"random_uniform\")\n",
        "    self.W3 = self.add_weight(shape = (self.words, self.embedding_size), initializer = \"random_uniform\")\n",
        "    self.W4 = self.add_weight(shape = (self.words, self.embedding_size), initializer = \"random_uniform\")\n",
        "    print()\n",
        "  def call(self,inputs):\n",
        "    query,value=inputs\n",
        "    #attention mechanism,o selectively focus on different parts of the input sequence when producing each element of the output sequence. It allows the model to\n",
        "    # weigh the relevance of each input element dynamically based on the context provided by the current state of the model.\n",
        "    regressed_query=tf.einsum(\"bi,ci->bi\",query,self.w1)\n",
        "    regressed_value = tf.einsum(\"bij, ij -> bij\", value, self.W2)\n",
        "    # While tf.einsum can indeed be used to compute dot products, it is a more general-purpose function that can perform a wide range of tensor operations beyond just dot products\n",
        "    sum_query_value=tf.einsum(\"bij,ij->bij\", regressed_query,regressed_value)\n",
        "    # hyperbolic tanget function which are from neural network\n",
        "    sum_of_query_value=tf.nn.tanh(sum_query_value)\n",
        "    a=tf.einsum(\"bij,ij->bij\",sum_of_query_value,self.W3)\n",
        "    # sum of elements in specified axis\n",
        "    a=tf.math.reduce_sum(a,axis=-1)\n",
        "    #sum of elements along the specified axis of the tensor a\n",
        "    a=tf.nn.softmax(a)\n",
        "    context = tf.einsum(\"bi, bij -> bij\", a, value)\n",
        "    context = tf.reduce_sum(context, axis = 1)\n",
        "\n",
        "\n",
        "    return context\n",
        "class AdditiveAttentionTranslator:\n",
        "    # Setting up some parameters for the translator\n",
        "    encoder_input_words = 20  # Number of words in the input sequence\n",
        "    vocab_size = 1000  # Size of the vocabulary\n",
        "    embedding_size = 128  # Size of the word embeddings\n",
        "    epochs = 30  # Number of training epochs\n",
        "    batch_size = 200  # Number of samples processed in each training iteration\n",
        "    optimizer = tf.keras.optimizers.Adam()  # Optimizer used for training\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  # Loss function used for training\n",
        "    loss_history = []  # List to store the training loss history\n",
        "\n",
        "    # Function to create encoder and decoder models\n",
        "    def get_enc_dec(self):\n",
        "        # Define input for encoder\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "        # Create an encoder using the defined input shape\n",
        "        encode = Encoder(vocab_size=self.vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        # Define the encoder model\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        # Define input for decoder\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        # Create embeddings for decoder input\n",
        "        x_decoder = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        # Define input states for decoder\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "        # Create a decoder using the defined inputs\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.vocab_size, words=self.encoder_input_words)(\n",
        "            (x_decoder[:, 0], x_state_input, x_states_input))\n",
        "        # Define the decoder model\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        # Return summaries of both encoder and decoder\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    # Function to generate random training data\n",
        "    def generate_random_data(self, instances=1000, decoder_words=10):\n",
        "        # Generate random input sequences and output sequences\n",
        "        X1, X2 = np.random.randint(self.vocab_size, size=(instances, self.encoder_input_words)), np.random.randint(\n",
        "            self.vocab_size, size=(instances, decoder_words))\n",
        "        # Generate random labels\n",
        "        Y = np.eye(self.vocab_size)[\n",
        "            np.random.choice(self.vocab_size, instances * decoder_words)].reshape(instances, decoder_words,\n",
        "                                                                                   self.vocab_size)\n",
        "        # Store generated data\n",
        "        self.X1, self.X2, self.Y = X1, X2, Y\n",
        "        # Return generated data\n",
        "        return X1, X2, Y\n",
        "\n",
        "    # Function to train the translator\n",
        "    def train_translator(self):\n",
        "        # Set logging level\n",
        "        tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "        # Get optimizer and loss function\n",
        "        optimizer, loss_fn = self.optimizer, self.loss_fn\n",
        "\n",
        "        # Get training parameters\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = tf.shape(self.Y)[0]\n",
        "\n",
        "        X1, X2, Y = self.X1, self.X2, self.Y\n",
        "\n",
        "        # Initialize loss history\n",
        "        self.loss_history = []\n",
        "\n",
        "        # Iterate over epochs\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            # Iterate over batches\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X1[batch:batch + batch_size]\n",
        "                    x2_train = X2[batch:batch + batch_size]\n",
        "                    y_train = Y[batch:batch + batch_size]\n",
        "\n",
        "                    # Encode input sequences\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    # Decode output sequences\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(y_train[:, query_number], output)\n",
        "                # Calculate gradients and update weights\n",
        "                grads = tape.gradient(loss_count,\n",
        "                                      self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(\n",
        "                    zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "            # Print epoch information\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    # Function to translate a given sentence\n",
        "    def translate_sentence(self, keys, query_start, query_size=None):\n",
        "        if query_size == None:\n",
        "            query_size = self.X2.shape[-1]\n",
        "        # Encode input sequence\n",
        "        H, state = self.encoder(keys)\n",
        "\n",
        "        value = []\n",
        "        state_steps = []\n",
        "        value.append(int(query_start[0][0]))\n",
        "\n",
        "        # Decode output sequence\n",
        "        for query_number in range(query_size):\n",
        "            output, state = self.decoder((query_start, state, H))\n",
        "            query_start = np.argmax(output.numpy(), axis=-1)\n",
        "            value.append(query_start[0])\n",
        "            state_steps.append(state)\n",
        "\n",
        "        return value, state_steps\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,embedding_size=128,vocab_size=1000,words=20):\n",
        "    super(Decoder,self).__init()\n",
        "    self.embedding_size=embedding_size\n",
        "    self.vocab_size=vocab_size\n",
        "    self.words=words\n",
        "  def build(self,input_shapes):\n",
        "    self.attention=BahdanauAttention(words = self.words, embedding_size = self.embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(self.embedding_size)\n",
        "    self.bi=tf.keras.layers.Bidirectional(self.gru,)\n",
        "    self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation = 'tanh')\n",
        "    self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation = 'tanh')\n",
        "    self.op3 = tf.keras.layers.Dense(self.vocab_size, activation = 'softmax')\n",
        "    print()\n",
        "  def call(self,inputs):\n",
        "    y,state,encode=inputs\n",
        "    context=self.attention((state,encode))\n",
        "    state_expanded = tf.expand_dims(state, axis = 1)\n",
        "    context_expanded = tf.expand_dims(context, axis = 1)\n",
        "    y_expanded = tf.expand_dims(y, axis = 1)\n",
        "\n",
        "    gru1_input = tf.concat([state_expanded, context_expanded], axis = 1)\n",
        "    gru1_input2 = tf.concat([gru1_input, y_expanded], axis = 1)\n",
        "    new_state =  self.bi(gru1_input2)\n",
        "    g_input = tf.concat([tf.concat([y, context], axis = -1), new_state], axis = -1)\n",
        "    g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "\n",
        "    return g_output, new_state\n",
        "\n"
      ],
      "metadata": {
        "id": "4bJpoUeVdUns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "class EnglishToTeluguTranslator:\n",
        "    def __init__(self, encoder_input_words=20, english_vocab_size=1000, telugu_vocab_size=1000, embedding_size=128,\n",
        "                 epochs=30, batch_size=200, optimizer='adam'):\n",
        "        self.encoder_input_words = encoder_input_words\n",
        "        self.english_vocab_size = english_vocab_size\n",
        "        self.telugu_vocab_size = telugu_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_history = []\n",
        "        self.encoder = encoder_model\n",
        "        self.decoder = None\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "        encode = Encoder(vocab_size=self.english_vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        x_decoder = Embedding(self.telugu_vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.telugu_vocab_size,\n",
        "                         words=self.encoder_input_words)((x_decoder[:, 0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def train_translator(self, X_english, X_telugu):\n",
        "        optimizer = tf.keras.optimizers.Adam()  # You can change the optimizer here if needed\n",
        "        loss_fn = self.loss_fn\n",
        "\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = len(X_english)\n",
        "\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X_english[batch:batch + batch_size]\n",
        "                    x2_train = X_telugu[batch:batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(x2_train[:, query_number], output)\n",
        "\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, english_sentence):\n",
        "        # Assuming you have a tokenizer initialized and fit on your English text data\n",
        "        # english_indices = your_tokenizer.texts_to_sequences([english_sentence])[0]\n",
        "        # english_indices = np.array([english_indices])\n",
        "\n",
        "        english_indices = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])  # Example input, replace with actual indices\n",
        "\n",
        "        H, state = self.encoder(english_indices)\n",
        "\n",
        "        telugu_sentence = []\n",
        "\n",
        "        start_token = np.array([2])  # Replace 2 with the actual index of the start token in the Telugu vocabulary\n",
        "        start_token = tf.expand_dims(start_token, axis=0)\n",
        "\n",
        "        for _ in range(10):  # Replace 10 with the actual maximum length of the Telugu sentence you want to generate\n",
        "            output, state = self.decoder((start_token, state, H))\n",
        "            predicted_word_index = np.argmax(output.numpy(), axis=-1)\n",
        "            telugu_sentence.append(predicted_word_index[0][0])\n",
        "\n",
        "            if predicted_word_index[0][0] == 3:  # Replace 3 with the actual index of the end token in the Telugu vocabulary\n",
        "                break\n",
        "\n",
        "            start_token = predicted_word_index\n",
        "\n",
        "        return telugu_sentence\n"
      ],
      "metadata": {
        "id": "XWDR5EXa23Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Luong Attention (Scaled Dot-Product Attention):\n",
        "* Self-Attention (Scaled Dot-Product Attention):"
      ],
      "metadata": {
        "id": "zqFb0bkmjxfN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "La_o1zTlzJE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Parse the text data\n",
        "with open(\"english_telugu_data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "english_sentences = []\n",
        "telugu_sentences = []\n",
        "for line in lines:\n",
        "    english, telugu = line.strip().split(\"++++$++++\")\n",
        "    english_sentences.append(english.strip())\n",
        "    telugu_sentences.append(telugu.strip())\n",
        "\n",
        "# Step 2: Tokenization\n",
        "english_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "telugu_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
        "\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "telugu_sequences = telugu_tokenizer.texts_to_sequences(telugu_sentences)\n",
        "\n",
        "# Step 3: Padding\n",
        "max_length = max(max(len(seq) for seq in english_sequences), max(len(seq) for seq in telugu_sequences))\n",
        "english_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, maxlen=max_length, padding='post')\n",
        "telugu_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(telugu_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Step 4: Data Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(english_sequences_padded, telugu_sequences_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Model Training\n",
        "translator = EnglishToTeluguTranslator()\n",
        "translator.train_translator(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluation (Optional)\n",
        "# Evaluate your model's performance on the validation set using metrics like BLEU score or simply by inspecting translations manually.\n",
        "\n",
        "# Step 7: Inference\n",
        "english_sentence = \"His legs are long.\"\n",
        "english_sequence = english_tokenizer.texts_to_sequences([english_sentence])\n",
        "translated_sequence = translator.translate_sentence(english_sequence)\n",
        "translated_sentence = telugu_tokenizer.sequences_to_texts([translated_sequence])[0]\n",
        "print(\"Translated Sentence:\", translated_sentence)\n"
      ],
      "metadata": {
        "id": "ZOiyYfF0jxJL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "8d5bf1cf-8845-4e02-c8d2-5a3e64550f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'english_telugu_data.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-44433154088c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Parse the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english_telugu_data.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menglish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'english_telugu_data.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9CbpihhPOJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "# from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# class Encoder(tf.keras.layers.Layer):\n",
        "#     def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "#         super(Encoder, self).__init__()\n",
        "#         self.vocab_size = vocab_size\n",
        "#         self.embedding_size = embedding_size\n",
        "\n",
        "#     def build(self, input_shapes):\n",
        "#         self.embedding_layer = Embedding(self.vocab_size, self.embedding_size)\n",
        "#         self.gru = GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         words = inputs\n",
        "#         embeddings = self.embedding_layer(words)\n",
        "#         output, state = self.gru(embeddings)\n",
        "#         return output, state\n",
        "\n",
        "# class BahdanauAttention(tf.keras.layers.Layer):\n",
        "#     def __init__(self, words=20, embedding_size=128):\n",
        "#         super(BahdanauAttention, self).__init__()\n",
        "#         self.words = words\n",
        "#         self.embedding_size = embedding_size\n",
        "\n",
        "#     def build(self, input_shapes):\n",
        "#         self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "#         self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "#         self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "#         self.W4 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         query, value = inputs\n",
        "\n",
        "#         regressed_query = tf.einsum(\"bi,ci -> bi\", query, self.W1)\n",
        "#         regressed_value = tf.einsum(\"bij, ij -> bij\", value, self.W2)\n",
        "\n",
        "#         sum_query_value = tf.einsum(\"bi, bji -> bji\", regressed_query, regressed_value)\n",
        "#         sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "\n",
        "#         a = tf.einsum(\"bij, ij -> bij\", sum_of_query_value, self.W3)\n",
        "#         a = tf.math.reduce_sum(a, axis=-1)\n",
        "#         a = tf.nn.softmax(a)\n",
        "\n",
        "#         context = tf.einsum(\"bi, bij -> bij\", a, value)\n",
        "#         context = tf.reduce_sum(context, axis=1)\n",
        "\n",
        "#         return context\n",
        "\n",
        "# class Decoder(tf.keras.layers.Layer):\n",
        "#     def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.embedding_size = embedding_size\n",
        "#         self.vocab_size = vocab_size\n",
        "#         self.words = words\n",
        "\n",
        "#     def build(self, input_shapes):\n",
        "#         self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "#         self.gru = GRU(self.embedding_size)\n",
        "#         self.op1 = Dense(self.embedding_size * 10, activation='tanh')\n",
        "#         self.op2 = Dense(self.embedding_size * 10, activation='tanh')\n",
        "#         self.op3 = Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         y, state, encode = inputs\n",
        "\n",
        "#         context = self.attention((state, encode))\n",
        "\n",
        "#         state_expanded = tf.expand_dims(state, axis=1)\n",
        "#         context_expanded = tf.expand_dims(context, axis=1)\n",
        "#         y_expanded = tf.expand_dims(y, axis=1)\n",
        "\n",
        "#         gru1_input = tf.concat([state_expanded, context_expanded], axis=1)\n",
        "#         gru1_input2 = tf.concat([gru1_input, y_expanded], axis=1)\n",
        "\n",
        "#         new_state = self.gru(gru1_input2)\n",
        "\n",
        "#         g_input = tf.concat([tf.concat([y, context], axis=-1), new_state], axis=-1)\n",
        "#         g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "\n",
        "#         return g_output, new_state\n",
        "\n",
        "# class EnglishToTeluguTranslator:\n",
        "#     def __init__(self, encoder_input_words=20, english_vocab_size=1000, telugu_vocab_size=1000, embedding_size=128,\n",
        "#                  epochs=30, batch_size=200, optimizer='adam'):\n",
        "#         self.encoder_input_words = encoder_input_words\n",
        "#         self.english_vocab_size = english_vocab_size\n",
        "#         self.telugu_vocab_size = telugu_vocab_size\n",
        "#         self.embedding_size = embedding_size\n",
        "#         self.epochs = epochs\n",
        "#         self.batch_size = batch_size\n",
        "#         self.optimizer = optimizer\n",
        "#         self.loss_fn = CategoricalCrossentropy(from_logits=True)\n",
        "#         self.loss_history = []\n",
        "#         self.encoder = None\n",
        "#         self.decoder = None\n",
        "\n",
        "#     def get_enc_dec(self):\n",
        "#         x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "#         encode = Encoder(vocab_size=self.english_vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "#         self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "#         x_decoder_input = tf.keras.layers.Input(1)\n",
        "#         x_decoder = Embedding(self.telugu_vocab_size, self.embedding_size)(x_decoder_input)\n",
        "#         x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "#         x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "#         decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.telugu_vocab_size,\n",
        "#                          words=self.encoder_input_words)((x_decoder[:, 0], x_state_input, x_states_input))\n",
        "#         self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "#         return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "#     def train_translator(self, X_english, X_telugu):\n",
        "#         optimizer = tf.keras.optimizers.Adam()  # You can change the optimizer here if needed\n",
        "#         loss_fn = self.loss_fn\n",
        "\n",
        "#         epochs, batch_size = self.epochs, self.batch_size\n",
        "#         total_instances = len(X_english)\n",
        "\n",
        "#         self.loss_history = []\n",
        "\n",
        "#         for epoch in range(epochs):\n",
        "#             batch_loss = tf.constant(0.0)\n",
        "#             for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "#                 with tf.GradientTape() as tape:\n",
        "#                     loss_count = tf.constant(0.0)\n",
        "#                     x1_train = X_english[batch:batch + batch_size]\n",
        "#                     x2_train = X_telugu[batch:batch + batch_size]\n",
        "\n",
        "#                     H, state = self.encoder(x1_train)\n",
        "\n",
        "#                     for query_number in range(x2_train.shape[-1]):\n",
        "#                         output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "#                         loss_count = loss_count + loss_fn(x2_train[:, query_number], output)\n",
        "\n",
        "#                 grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "#                 optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "#                 batch_loss = batch_loss + loss_count\n",
        "\n",
        "#             print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "#             self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "#     def translate_sentence(self, english_sentence):\n",
        "#         english_indices = your_tokenizer.texts_to_sequences([english_sentence])[0]\n",
        "#         english_indices = np.array([english_indices])\n",
        "#         H, state = self.encoder(x1_train, initial_state=self.encoder.initial_state)\n",
        "#         telugu_sentence = []\n",
        "\n",
        "#         start_token = np.array([2])  # Replace 2 with the actual index of the start token in the Telugu vocabulary\n",
        "#         start_token = tf.expand_dims(start_token, axis=0)\n",
        "\n",
        "#         for _ in range(10):  # Replace 10 with the actual maximum length of the Telugu sentence you want to generate\n",
        "#             output, state = self.decoder((start_token, state, H))\n",
        "#             predicted_word_index = np.argmax(output.numpy(), axis=-1)\n",
        "#             telugu_sentence.append(predicted_word_index[0][0])\n",
        "\n",
        "#             if predicted_word_index[0][0] == 3:  # Replace 3 with the actual index of the end token in the Telugu vocabulary\n",
        "#                 break\n",
        "\n",
        "#             start_token = predicted_word_index\n",
        "\n",
        "#         return telugu_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "uUDNquw1Rpdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "        self.bi = tf.keras.layers.Bidirectional(self.gru)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output_sequence, forward_state, backward_state = self.bi(embeddings)\n",
        "        return (output_sequence, forward_state, backward_state)\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words=20, embedding_size=128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W4 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "        regressed_query = tf.einsum(\"bi,ci->bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij, ij -> bij\", value, self.W2)\n",
        "        sum_query_value = tf.einsum(\"bij,ij->bij\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "        a = tf.einsum(\"bij,ij->bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.math.reduce_sum(a, axis=-1)\n",
        "        a = tf.nn.softmax(a)\n",
        "        context = tf.einsum(\"bi, bij -> bij\", a, value)\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size)\n",
        "        self.bi = tf.keras.layers.Bidirectional(self.gru)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "        context = self.attention((state, encode))\n",
        "        state_expanded = tf.expand_dims(state, axis=1)\n",
        "        context_expanded = tf.expand_dims(context, axis=1)\n",
        "        y_expanded = tf.expand_dims(y, axis=1)\n",
        "        gru1_input = tf.concat([state_expanded, context_expanded], axis=1)\n",
        "        gru1_input2 = tf.concat([gru1_input, y_expanded], axis=1)\n",
        "        new_state = self.bi(gru1_input2)\n",
        "        g_input = tf.concat([tf.concat([y, context], axis=-1), new_state], axis=-1)\n",
        "        g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "        return g_output, new_state\n",
        "\n",
        "# Define the Encoder class and other custom layers as given in the code snippet\n",
        "\n",
        "# Define the EnglishToTeluguTranslator class with corrections\n",
        "class EnglishToTeluguTranslator:\n",
        "    def __init__(self, encoder_input_words=20, english_vocab_size=1000, telugu_vocab_size=1000, embedding_size=128,\n",
        "                 epochs=30, batch_size=200, optimizer='adam'):\n",
        "        self.encoder_input_words = encoder_input_words\n",
        "        self.english_vocab_size = english_vocab_size\n",
        "        self.telugu_vocab_size = telugu_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_history = []\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.english_tokenizer = Tokenizer(num_words=english_vocab_size, oov_token='<OOV>')\n",
        "        self.telugu_tokenizer = Tokenizer(num_words=telugu_vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    def tokenize_sentences(self, english_sentences, telugu_sentences):\n",
        "        english_sentences = [str(sentence) for sentence in english_sentences]\n",
        "        telugu_sentences = [str(sentence) for sentence in telugu_sentences]\n",
        "\n",
        "        self.english_tokenizer.fit_on_texts(english_sentences)\n",
        "        self.telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
        "\n",
        "        X_english = self.english_tokenizer.texts_to_sequences(english_sentences)\n",
        "        X_telugu = self.telugu_tokenizer.texts_to_sequences(telugu_sentences)\n",
        "\n",
        "        return X_english, X_telugu\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "        encode = Encoder(vocab_size=self.english_vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(shape=(1,))\n",
        "        x_decoder = Embedding(self.telugu_vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        x_state_input = tf.keras.layers.Input(shape=(self.embedding_size,))\n",
        "        x_states_input = tf.keras.layers.Input(shape=(self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.telugu_vocab_size,\n",
        "                         words=self.encoder_input_words)((x_decoder[:, 0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def train_translator(self, X_english, X_telugu):\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        loss_fn = self.loss_fn\n",
        "\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = len(X_english)\n",
        "\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X_english[batch:batch + batch_size]\n",
        "                    x2_train = X_telugu[batch:batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(x2_train[:, query_number], output)\n",
        "\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, english_sentence):\n",
        "        english_indices = self.english_tokenizer.texts_to_sequences([english_sentence])\n",
        "        english_indices = np.array(english_indices)\n",
        "\n",
        "        H, state = self.encoder(english_indices)\n",
        "\n",
        "        telugu_sentence = []\n",
        "\n",
        "        start_token = np.array([[2]])  # Start token index\n",
        "        start_token = tf.expand_dims(start_token, axis=0)\n",
        "\n",
        "        for _ in range(10):  # Max length of the Telugu sentence\n",
        "            output, state = self.decoder((start_token, state, H))\n",
        "            predicted_word_index = np.argmax(output.numpy(), axis=-1)\n",
        "            telugu_sentence.append(predicted_word_index[0][0])\n",
        "\n",
        "            if predicted_word_index[0][0] == 3:  # End token index\n",
        "                break\n",
        "\n",
        "            start_token = predicted_word_index\n",
        "\n",
        "        return telugu_sentence\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# Initialize the translator\n",
        "translator = EnglishToTeluguTranslator()\n",
        "\n",
        "# Initialize encoder and decoder\n",
        "translator.get_enc_dec()\n",
        "\n",
        "# Tokenize sentences\n",
        "X_train_eng_seq, X_train_tel_seq = translator.tokenize_sentences(X_train_eng, X_train_tel)\n",
        "\n",
        "# Train the translator\n",
        "translator.train_translator(X_train_eng_seq, X_train_tel_seq)\n",
        "\n",
        "# Translate a sample English sentence\n",
        "sample_english_sentence = \"Hello, how are you?\"\n",
        "translated_telugu_sentence = translator.translate_sentence(sample_english_sentence)\n",
        "print(\"Translated Telugu sentence:\", translated_telugu_sentence)\n"
      ],
      "metadata": {
        "id": "iAtX-BOTWQen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "d2eb3a9e-0a28-4be5-e9a9-00de930327bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ce1fb2ba38bd>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;31m# Initialize the translator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnglishToTeluguTranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# Initialize encoder and decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-ce1fb2ba38bd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoder_input_words, english_vocab_size, telugu_vocab_size, embedding_size, epochs, batch_size, optimizer)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menglish_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menglish_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<OOV>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtelugu_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtelugu_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<OOV>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"/content/engtotel.xlsx\")\n",
        "\n",
        "# Split the dataset into English and Telugu sentences\n",
        "english_sentences = data[\"english\"].values\n",
        "telugu_sentences = data[\"telugu\"].values\n",
        "\n",
        "# Tokenize the English sentences (assuming you have a tokenizer)\n",
        "# english_tokenizer = ...\n",
        "\n",
        "# Convert English sentences to sequences of indices\n",
        "# X_english = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Assume X_english is ready with English sequences, similarly prepare X_telugu\n",
        "english_sentences = data[\"english\"].values\n",
        "telugu_sentences = data[\"telugu\"].values\n",
        "\n",
        "# Tokenize the English sentences (assuming you have a tokenizer)\n",
        "# english_tokenizer = ...\n",
        "\n",
        "# Convert English sentences to sequences of indices\n",
        "X_english = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Assume X_english is ready with English sequences, similarly prepare X_telugu\n",
        "X_telugu= english_tokenizer.texts_to_sequences(english_sentences)\n",
        "# Split the data into training and testing sets\n",
        "X_train_eng, X_test_eng, X_train_tel, X_test_tel = train_test_split(X_english, X_telugu, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the translator\n",
        "translator = EnglishToTeluguTranslator()\n",
        "\n",
        "# Train the translator\n",
        "translator.train_translator(X_train_eng, X_train_tel)\n",
        "\n",
        "# Translate a sample English sentence\n",
        "sample_english_sentence = \"Hello, how are you?\"\n",
        "translated_telugu_sentence = translator.translate_sentence(sample_english_sentence)\n",
        "print(\"Translated Telugu sentence:\", translated_telugu_sentence)"
      ],
      "metadata": {
        "id": "23PuORGX7aet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/engtotel.xlsx\")\n",
        "english_sentences = data[\"english\"].values\n",
        "telugu_sentences = data[\"telugu\"].values"
      ],
      "metadata": {
        "id": "-LZAwIoz9FjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6udl1XzS7pDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "        self.bi = tf.keras.layers.Bidirectional(self.gru)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output_sequence, forward_state, backward_state = self.bi(embeddings)\n",
        "        return (output_sequence, forward_state, backward_state)\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words=20, embedding_size=128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "        regressed_query = tf.einsum(\"bi,ci->bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij,ij->bij\", value, self.W2)\n",
        "        sum_query_value = tf.einsum(\"bij,ij->bij\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "        a = tf.einsum(\"bij,ij->bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.math.reduce_sum(a, axis=-1)\n",
        "        a = tf.nn.softmax(a)\n",
        "        context = tf.einsum(\"bi,bij->bj\", a, value)\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "        context = self.attention((state, encode))\n",
        "        y = tf.expand_dims(y, axis=1)\n",
        "        y_context = tf.concat([y, context], axis=-1)\n",
        "        gru_input = tf.concat([y_context, state[:, None]], axis=-1)\n",
        "        gru_output, new_state = self.gru(gru_input)\n",
        "        output = self.op3(self.op2(self.op1(gru_output)))\n",
        "        return output, new_state\n",
        "\n",
        "class EnglishToTeluguTranslator:\n",
        "    def __init__(self, encoder_input_words=20, english_vocab_size=1000, telugu_vocab_size=1000, embedding_size=128,\n",
        "                 epochs=30, batch_size=200, optimizer='adam'):\n",
        "        self.encoder_input_words = encoder_input_words\n",
        "        self.english_vocab_size = english_vocab_size\n",
        "        self.telugu_vocab_size = telugu_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_history = []\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.english_tokenizer = Tokenizer(num_words=english_vocab_size, oov_token='<OOV>')\n",
        "        self.telugu_tokenizer = Tokenizer(num_words=telugu_vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    def tokenize_sentences(self, english_sentences, telugu_sentences):\n",
        "        english_sentences = [str(sentence) for sentence in english_sentences]\n",
        "        telugu_sentences = [str(sentence) for sentence in telugu_sentences]\n",
        "\n",
        "        self.english_tokenizer.fit_on_texts(english_sentences)\n",
        "        self.telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
        "\n",
        "        X_english = self.english_tokenizer.texts_to_sequences(english_sentences)\n",
        "        X_telugu = self.telugu_tokenizer.texts_to_sequences(telugu_sentences)\n",
        "\n",
        "        return X_english, X_telugu\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "        encode = Encoder(vocab_size=self.english_vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "        x_state_input = tf.keras.layers.Input(shape=(self.embedding_size,))\n",
        "        x_states_input = tf.keras.layers.Input(shape=(self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.telugu_vocab_size,\n",
        "                         words=self.encoder_input_words)((x_decoder_input, x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def train_translator(self, X_english, X_telugu):\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        loss_fn = self.loss_fn\n",
        "\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = len(X_english)\n",
        "\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X_english[batch:batch + batch_size]\n",
        "                    x2_train = X_telugu[batch:batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(x2_train[:, query_number], output)\n",
        "\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable))\n",
        "                        print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "        self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "def translate_sentence(self, english_sentence):\n",
        "    english_indices = self.english_tokenizer.texts_to_sequences([english_sentence])\n",
        "    english_indices = np.array(english_indices)\n",
        "\n",
        "    H, state = self.encoder(english_indices)\n",
        "\n",
        "    telugu_sentence = []\n",
        "\n",
        "    start_token = np.array([[2]])  # Start token index\n",
        "    start_token = tf.expand_dims(start_token, axis=0)\n",
        "\n",
        "    for _ in range(10):  # Max length of the Telugu sentence\n",
        "        output, state = self.decoder((start_token, state, H))\n",
        "        predicted_word_index = np.argmax(output.numpy(), axis=-1)\n",
        "        telugu_sentence.append(predicted_word_index[0][0])\n",
        "\n",
        "        if predicted_word_index[0][0] == 3:  # End token index\n",
        "            break\n",
        "\n",
        "        start_token = predicted_word_index\n",
        "\n",
        "    return telugu_sentence\n"
      ],
      "metadata": {
        "id": "GJvsYi798SjX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "d9340396-8275-440b-ea4e-a85e36965a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-14-b71613ca68a6>, line 143)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-b71613ca68a6>\"\u001b[0;36m, line \u001b[0;32m143\u001b[0m\n\u001b[0;31m    print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jzvE-xyQBV7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "        self.bi = tf.keras.layers.Bidirectional(self.gru)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output_sequence, forward_state, backward_state = self.bi(embeddings)\n",
        "        return (output_sequence, forward_state, backward_state)\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words=20, embedding_size=128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "        regressed_query = tf.einsum(\"bi,ci->bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij,ij->bij\", tf.expand_dims(value, 1), self.W2)\n",
        "        sum_query_value = tf.einsum(\"bij,ij->bij\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "        a = tf.einsum(\"bij,ij->bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.math.reduce_sum(a, axis=-1)\n",
        "        a = tf.nn.softmax(a)\n",
        "        context = tf.einsum(\"bi,bij->bj\", a, value)\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "        context = self.attention((state, encode))\n",
        "        y = tf.expand_dims(y, axis=1)\n",
        "        y_context = tf.concat([y, context], axis=-1)\n",
        "        gru_input = tf.concat([y_context, state[:, None]], axis=-1)\n",
        "        gru_output, new_state = self.gru(gru_input)\n",
        "        output = self.op3(self.op2(self.op1(gru_output)))\n",
        "        return output, new_state\n",
        "\n",
        "class EnglishToTeluguTranslator:\n",
        "    def __init__(self, encoder_input_words=20, english_vocab_size=1000, telugu_vocab_size=1000, embedding_size=128,\n",
        "                 epochs=30, batch_size=200, optimizer='adam'):\n",
        "        self.encoder_input_words = encoder_input_words\n",
        "        self.english_vocab_size = english_vocab_size\n",
        "        self.telugu_vocab_size = telugu_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = CategoricalCrossentropy(from_logits=True)\n",
        "        self.loss_history = []\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.english_tokenizer = Tokenizer(num_words=english_vocab_size, oov_token='<OOV>')\n",
        "        self.telugu_tokenizer = Tokenizer(num_words=telugu_vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    def tokenize_sentences(self, english_sentences, telugu_sentences):\n",
        "        english_sentences = [str(sentence) for sentence in english_sentences]\n",
        "        telugu_sentences = [str(sentence) for sentence in telugu_sentences]\n",
        "\n",
        "        self.english_tokenizer.fit_on_texts(english_sentences)\n",
        "        self.telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
        "\n",
        "        X_english = self.english_tokenizer.texts_to_sequences(english_sentences)\n",
        "        X_telugu = self.telugu_tokenizer.texts_to_sequences(telugu_sentences)\n",
        "\n",
        "        return X_english, X_telugu\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "        encode = Encoder(vocab_size=self.english_vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "        x_state_input = tf.keras.layers.Input(shape=(self.embedding_size,))\n",
        "        x_states_input = tf.keras.layers.Input(shape=(self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.telugu_vocab_size,\n",
        "                         words=self.encoder_input_words)((x_decoder_input, x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def train_translator(self, X_english, X_telugu):\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        loss_fn = self.loss_fn\n",
        "\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = len(X_english)\n",
        "\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X_english[batch:batch + batch_size]\n",
        "                    x2_train = X_telugu[batch:batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(x2_train[:, query_number], output)\n",
        "\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "    def translate_sentence(self, english_sentence):\n",
        "        english_indices = self.english_tokenizer.texts_to_sequences([english_sentence])\n",
        "        english_indices = np.array(english_indices)\n",
        "\n",
        "        H, state = self.encoder(english_indices)\n",
        "\n",
        "        telugu_sentence = []\n",
        "\n",
        "        start_token = np.array([[2]])  # Start token index\n",
        "        start_token = tf.expand_dims(start_token, axis=0)\n",
        "\n",
        "        for _ in range(10):  # Max length of the Telugu sentence\n",
        "            output, state = self.decoder((start_token, state, H))\n",
        "            predicted_word_index = np.argmax(output.numpy(), axis=-1)\n",
        "            telugu_sentence.append(predicted_word_index[0][0])\n",
        "\n",
        "            if predicted_word_index[0][0] == 3:  # End token index\n",
        "                break\n",
        "\n",
        "            start_token = predicted_word_index\n",
        "\n",
        "        return telugu_sentence\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel(\"/content/engtotel.xlsx\")\n",
        "\n",
        "# Split the dataset into English and Telugu sentences\n",
        "english_sentences = data[\"english\"].values\n",
        "telugu_sentences = data[\"telugu\"].values\n",
        "\n",
        "# Convert Telugu sentences to strings (assuming they are currently floats)\n",
        "telugu_sentences = telugu_sentences.astype(str)\n",
        "\n",
        "# Tokenize the English sentences (assuming you have a tokenizer)\n",
        "english_tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "\n",
        "# Convert English sentences to sequences of indices\n",
        "X_english = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Tokenize the Telugu sentences (assuming you have a tokenizer)\n",
        "telugu_tokenizer = Tokenizer(num_words=1000, oov_token='<OOV>')\n",
        "telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
        "\n"
      ],
      "metadata": {
        "id": "2P_2qX8rBs91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the translator\n",
        "translator = EnglishToTeluguTranslator()\n",
        "\n",
        "# Initialize encoder and decoder\n",
        "translator.get_enc_dec()\n",
        "\n",
        "# Train the translator\n",
        "translator.train_translator(X_train_eng, X_train_tel)\n",
        "\n",
        "# Translate a sample English sentence\n",
        "sample_english_sentence = \"Hello, how are you?\"\n",
        "translated_telugu_sentence = translator.translate_sentence(sample_english_sentence)\n",
        "print(\"Translated Telugu sentence:\", translated_telugu_sentence)\n"
      ],
      "metadata": {
        "id": "BPn_-5bYBx8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "# Download nltk resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size = 1000, embedding_size = 128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "    def build(self, input_shapes):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences = True, return_state = True)\n",
        "        print()\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output, state = self.gru(embeddings)\n",
        "        return (output, state)\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words = 20, embedding_size = 128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape = (1, self.embedding_size), initializer = \"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape = (self.words, self.embedding_size), initializer = \"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape = (self.words, self.embedding_size), initializer = \"random_uniform\")\n",
        "        self.W4 = self.add_weight(shape = (self.words, self.embedding_size), initializer = \"random_uniform\")\n",
        "        print()\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "\n",
        "        regressed_query = tf.einsum(\"bi,ci -> bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij, ij -> bij\", value, self.W2)\n",
        "\n",
        "        sum_query_value = tf.einsum(\"bi, bji -> bji\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "\n",
        "        a = tf.einsum(\"bij, ij -> bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.math.reduce_sum(a, axis = -1)\n",
        "        a = tf.nn.softmax(a)\n",
        "\n",
        "        context = tf.einsum(\"bi, bij -> bij\", a, value)\n",
        "        context = tf.reduce_sum(context, axis = 1)\n",
        "\n",
        "\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size = 128, vocab_size = 1000, words = 20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words = self.words, embedding_size = self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation = 'tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation = 'tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation = 'softmax')\n",
        "        print()\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "\n",
        "        context = self.attention((state, encode))\n",
        "\n",
        "        state_expanded = tf.expand_dims(state, axis = 1)\n",
        "        context_expanded = tf.expand_dims(context, axis = 1)\n",
        "        y_expanded = tf.expand_dims(y, axis = 1)\n",
        "\n",
        "        gru1_input = tf.concat([state_expanded, context_expanded], axis = 1)\n",
        "        gru1_input2 = tf.concat([gru1_input, y_expanded], axis = 1)\n",
        "\n",
        "        new_state = self.gru(gru1_input2)\n",
        "\n",
        "        g_input = tf.concat([tf.concat([y, context], axis = -1), new_state], axis = -1)\n",
        "        g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "\n",
        "        return g_output, new_state\n",
        "\n",
        "class AdditiveAttentionTranslator:\n",
        "    encoder_input_words = 20\n",
        "    vocab_size = 1000\n",
        "    embedding_size = 128\n",
        "    epochs = 10\n",
        "    batch_size = 200\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
        "    loss_history = []\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "\n",
        "        encode = encode = Encoder(vocab_size = self.vocab_size, embedding_size = self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        x_decoder = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size = self.embedding_size, vocab_size = self.vocab_size, words = self.encoder_input_words)((x_decoder[:,0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs = decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def generate_random_data(self, instances = 1000, decoder_words = 10):\n",
        "        X1, X2 = np.random.randint(self.vocab_size, size=(instances, self.encoder_input_words)), np.random.randint(self.vocab_size, size=(instances, decoder_words))\n",
        "        Y = Y = np.eye(self.vocab_size)[np.random.choice(self.vocab_size, instances * decoder_words)].reshape(instances, decoder_words, self.vocab_size)\n",
        "        self.X1, self.X2, self.Y = X1, X2, Y\n",
        "        return X1, X2, Y\n",
        "\n",
        "    def train_translator(self):\n",
        "        tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "        optimizer, loss_fn = self.optimizer, self.loss_fn\n",
        "\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = tf.shape(self.Y)[0]\n",
        "\n",
        "        X1, X2, Y = self.X1, self.X2, self.Y\n",
        "\n",
        "        self.get_enc_dec()\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X1[batch : batch + batch_size]\n",
        "                    x2_train = X2[batch : batch + batch_size]\n",
        "                    y_train = Y[batch : batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(y_train[:, query_number], output)\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, keys, query_start, query_size = None):\n",
        "        if query_size == None:\n",
        "            query_size = self.X2.shape[-1]\n",
        "        H, state = self.encoder(keys)\n",
        "\n",
        "        value = []\n",
        "        state_steps = []\n",
        "        value.append(int(query_start[0][0]))\n",
        "\n",
        "\n",
        "        for query_number in range(query_size):\n",
        "            output, state = self.decoder((query_start, state, H))\n",
        "            query_start = np.argmax(output.numpy(), axis = -1)\n",
        "            value.append(query_start[0])\n",
        "            state_steps.append(state)\n",
        "\n",
        "        return value, state_steps\n",
        "    def preprocess_input_sentence(self, input_sentence):\n",
        "        # Tokenize the input sentence using nltk word_tokenize\n",
        "        tokens = word_tokenize(input_sentence)\n",
        "\n",
        "        # Limit the number of tokens to the maximum input length\n",
        "        max_input_length = self.encoder_input_words\n",
        "        tokens = tokens[:max_input_length]\n",
        "\n",
        "        # Convert tokens to indices using a pretrained tokenizer\n",
        "        input_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        return input_indices\n",
        "\n",
        "    def postprocess_output(self, translated_indices):\n",
        "        # Convert translated indices to tokens using the same tokenizer used for preprocessing\n",
        "        translated_tokens = self.tokenizer.convert_ids_to_tokens(translated_indices)\n",
        "\n",
        "        # Join tokens into a sentence\n",
        "        translated_sentence = ' '.join(translated_tokens)\n",
        "\n",
        "        return translated_sentence\n"
      ],
      "metadata": {
        "id": "6hW4ut-xFO8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b8f5f3-c676-406f-e8f5-7b86572000a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data from Excel\n",
        "data = pd.read_excel('/content/engtotel.xlsx')\n",
        "\n",
        "# Assuming the first column is input and the second is output\n",
        "input_sentences = data.iloc[:, 0].tolist()\n",
        "output_sentences = data.iloc[:, 1].tolist()\n"
      ],
      "metadata": {
        "id": "5OI2d_4tRKZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have instantiated your AdditiveAttentionTranslator class as translator\n",
        "translator = AdditiveAttentionTranslator()\n",
        "translator.generate_random_data()  # Generate random data for training\n",
        "translator.train_translator()  # Train the translator\n"
      ],
      "metadata": {
        "id": "tFl2viVCR6Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Download nltk resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output, state = self.gru(embeddings)\n",
        "        return (output, state)\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words=20, embedding_size=128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W4 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "\n",
        "        regressed_query = tf.einsum(\"bi,ci -> bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij, ij -> bij\", value, self.W2)\n",
        "\n",
        "        sum_query_value = tf.einsum(\"bi, bji -> bji\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "\n",
        "        a = tf.einsum(\"bij, ij -> bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.math.reduce_sum(a, axis=-1)\n",
        "        a = tf.nn.softmax(a)\n",
        "\n",
        "        context = tf.einsum(\"bi, bij -> bij\", a, value)\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "\n",
        "        context = self.attention((state, encode))\n",
        "\n",
        "        state_expanded = tf.expand_dims(state, axis=1)\n",
        "        context_expanded = tf.expand_dims(context, axis=1)\n",
        "        y_expanded = tf.expand_dims(y, axis=1)\n",
        "\n",
        "        gru1_input = tf.concat([state_expanded, context_expanded], axis=1)\n",
        "        gru1_input2 = tf.concat([gru1_input, y_expanded], axis=1)\n",
        "\n",
        "        new_state = self.gru(gru1_input2)\n",
        "\n",
        "        g_input = tf.concat([tf.concat([y, context], axis=-1), new_state], axis=-1)\n",
        "        g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "\n",
        "        return g_output, new_state\n",
        "\n",
        "class AdditiveAttentionTranslator:\n",
        "    encoder_input_words = 20\n",
        "    vocab_size = 1000\n",
        "    embedding_size = 128\n",
        "    epochs = 10\n",
        "    batch_size = 200\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_history = []\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        # Load the tokenizer from a file or create a new one\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "        encode = Encoder(vocab_size=self.vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.vocab_size, words=self.encoder_input_words)((x_decoder[:, 0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def generate_random_data(self, instances=1000, decoder_words=10):\n",
        "        X1 = np.random.randint(self.vocab_size, size=(instances, self.encoder_input_words))\n",
        "        X2 = np.random.randint(self.vocab_size, size=(instances, decoder_words))\n",
        "        Y = np.eye(self.vocab_size)[np.random.choice(self.vocab_size, instances * decoder_words)].reshape(instances, decoder_words, self.vocab_size)\n",
        "        self.X1, self.X2, self.Y = X1, X2, Y\n",
        "        return X1, X2, Y\n",
        "\n",
        "    def train_translator(self):\n",
        "        tf.get_logger().setLevel('ERROR')\n",
        "        optimizer, loss_fn = self.optimizer, self.loss_fn\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = tf.shape(self.Y)[0]\n",
        "\n",
        "        X1, X2, Y = self.X1, self.X2, self.Y\n",
        "\n",
        "        self.get_enc_dec()\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X1[batch: batch + batch_size]\n",
        "                    x2_train = X2[batch: batch + batch_size]\n",
        "                    y_train = Y[batch: batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(y_train[:, query_number], output)\n",
        "\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, keys, query_start, query_size=None):\n",
        "        if query_size is None:\n",
        "            query_size = self.X2.shape[-1]\n",
        "        H, state = self.encoder(keys)\n",
        "        value = []\n",
        "        state_steps = []\n",
        "        value.append(int(query_start[0][0]))\n",
        "\n",
        "        for query_number in range(query_size):\n",
        "            output, state = self.decoder((query_start, state, H))\n",
        "            query_start = np.argmax(output.numpy(), axis=-1)\n",
        "            value.append(query_start[0])\n",
        "            state_steps.append(state)\n",
        "\n",
        "        return value, state_steps\n",
        "\n",
        "    def preprocess_input_sentence(self, input_sentence):\n",
        "        tokens = word_tokenize(input_sentence)\n",
        "        max_input_length = self.encoder_input_words\n",
        "        tokens = tokens[:max_input_length]\n",
        "        input_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        return input_indices\n",
        "\n",
        "    def translate_sentence(self, input_indices):\n",
        "        H, state = self.encoder(input_indices)\n",
        "\n",
        "        translated_sentence = []\n",
        "        state_steps = []\n",
        "        translated_sentence.append(input_indices[0])\n",
        "\n",
        "        for _ in range(self.encoder_input_words):\n",
        "            output, state = self.decoder(([translated_sentence[-1]], state, H))\n",
        "            translated_index = tf.argmax(output, axis=-1).numpy()[0]\n",
        "            translated_sentence.append(translated_index)\n",
        "            state_steps.append(state)\n",
        "\n",
        "        return translated_sentence[1:], state_steps\n",
        "\n",
        "    def translate_input(self, input_sentence):\n",
        "        # Preprocess input sentence (tokenization, padding, etc.)\n",
        "        input_indices = self.preprocess_input_sentence(input_sentence)\n",
        "\n",
        "        # Translate input sentence using the trained model\n",
        "        translated_sentence, _ = self.translate_sentence(input_indices)\n",
        "\n",
        "        # Post-process translated sentence\n",
        "        translated_sentence = self.postprocess_output(translated_sentence)\n",
        "\n",
        "        return translated_sentence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jrlsrQv4gIsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sample_english_sentence = \"Hello, how are you?\"\n",
        "translated_telugu_sentence = translate_input(sample_english_sentence)\n",
        "print(\"Translated Telugu sentence:\", translated_telugu_sentence)\n"
      ],
      "metadata": {
        "id": "nGoZMGUiUNIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already defined the AdditiveAttentionTranslator class\n",
        "\n",
        "# Create an instance of AdditiveAttentionTranslator\n",
        "translator = AdditiveAttentionTranslator()\n",
        "\n",
        "# Sample English sentence\n",
        "sample_english_sentence = \"Hello, how are you?\"\n",
        "\n",
        "# Translate the input sentence\n",
        "translated_telugu_sentence = translator.translate_input(sample_english_sentence)\n",
        "\n",
        "# Print the translated Telugu sentence\n",
        "print(\"Translated Telugu sentence:\", translated_telugu_sentence)\n"
      ],
      "metadata": {
        "id": "MmogeYtp2XaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data from Excel file\n",
        "file_path = \"/content/engtotel.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Create a mapping dictionary from English to Telugu using data from Excel\n",
        "english_to_telugu_map = dict(zip(df['english'], df['telugu']))\n",
        "\n",
        "def translate_english_to_telugu(english_sentence):\n",
        "    return english_to_telugu_map.get(english_sentence, \"Translation Not Available\")\n",
        "\n",
        "# Example usage:\n",
        "english_sentence = \"Smoke filled the room\"\n",
        "translated_sentence = translate_english_to_telugu(english_sentence)\n",
        "print(\"Translated Sentence:\", translated_sentence)\n",
        "\n",
        "# Preprocess the data for model training\n",
        "english_column_name = 'english'  # Replace 'English' with the actual English column name\n",
        "telugu_column_name = 'telugu'  # Replace 'Telugu' with the actual Telugu column name\n",
        "\n",
        "english_sentences = df[english_column_name].apply(lambda x: str(x)).tolist()\n",
        "telugu_sentences = df[telugu_column_name].apply(lambda x: str(x)).tolist()\n",
        "\n",
        "english_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "telugu_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "telugu_tokenizer.fit_on_texts(telugu_sentences)\n",
        "\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "telugu_sequences = telugu_tokenizer.texts_to_sequences(telugu_sentences)\n",
        "\n",
        "max_length = max(max(len(seq) for seq in english_sequences), max(len(seq) for seq in telugu_sequences))\n",
        "english_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, maxlen=max_length, padding='post')\n",
        "telugu_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(telugu_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(english_sequences_padded, telugu_sequences_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "class TranslatorModel(tf.keras.Model):\n",
        "    def _init_(self, input_vocab_size, output_vocab_size, embedding_size=128):\n",
        "        super(TranslatorModel, self)._init_()\n",
        "        self.encoder = tf.keras.layers.Embedding(input_vocab_size, embedding_size)\n",
        "        self.decoder = tf.keras.layers.Embedding(output_vocab_size, embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(embedding_size, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(output_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "        encoder_embeddings = self.encoder(encoder_inputs)\n",
        "        decoder_embeddings = self.decoder(decoder_inputs)\n",
        "\n",
        "        encoder_outputs, encoder_states = self.gru(encoder_embeddings)\n",
        "        decoder_outputs, _ = self.gru(decoder_embeddings, initial_state=encoder_states)\n",
        "\n",
        "        output = self.dense(decoder_outputs)\n",
        "        return output\n",
        "\n",
        "input_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "output_vocab_size = len(telugu_tokenizer.word_index) + 1\n",
        "\n",
        "translator_model = TranslatorModel(input_vocab_size, output_vocab_size)\n",
        "\n",
        "# Compile and train the model\n",
        "translator_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "translator_model.fit([X_train, y_train[:, :-1]], y_train[:, 1:], epochs=10, validation_data=([X_val, y_val[:, :-1]], y_val[:, 1:]))\n",
        "\n",
        "# Function to translate English sentence to Telugu using the trained model\n",
        "def translate_using_model(english_sentence):\n",
        "    english_sequence = english_tokenizer.texts_to_sequences([english_sentence])\n",
        "    english_sequence_padded = tf.keras.preprocessing.sequence.pad_sequences(english_sequence, maxlen=max_length, padding='post')\n",
        "    translated_sequence = np.argmax(translator_model.predict([english_sequence_padded, np.zeros((len(english_sequence_padded), max_length - 1))]), axis=-1)\n",
        "    translated_sentence = telugu_tokenizer.sequences_to_texts(translated_sequence)\n",
        "    return translated_sentence[0]\n",
        "\n",
        "# Example usage of the translation function\n",
        "english_sentence = \"His legs are long.\"\n",
        "translated_sentence = translate_using_model(english_sentence)\n",
        "print(\"Translated Sentence:\", translated_sentence)"
      ],
      "metadata": {
        "id": "7XBbb_u8Ub44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "3f551d52-88ec-4b70-9070-994d3c496a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Sentence: పొగ గదిని నింపింది\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 15292",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-42669abdeeb8>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0moutput_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtelugu_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtranslator_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslatorModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Compile and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 15292"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd  # Import pandas\n",
        "\n",
        "# Load data from Excel file\n",
        "file_path = \"/content/engtotel.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "english_to_telugu_map = dict(zip(df['english'], df['telugu']))\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size=1000, embedding_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        words = inputs\n",
        "        embeddings = self.embedding_layer(words)\n",
        "        output, state = self.gru(embeddings)\n",
        "        return (output, state)\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, words=20, embedding_size=128):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.words = words\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.W1 = self.add_weight(shape=(1, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W2 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W3 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "        self.W4 = self.add_weight(shape=(self.words, self.embedding_size), initializer=\"random_uniform\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "\n",
        "        regressed_query = tf.einsum(\"bi,ci -> bi\", query, self.W1)\n",
        "        regressed_value = tf.einsum(\"bij, ij -> bij\", value, self.W2)\n",
        "\n",
        "        sum_query_value = tf.einsum(\"bi, bji -> bji\", regressed_query, regressed_value)\n",
        "        sum_of_query_value = tf.nn.tanh(sum_query_value)\n",
        "\n",
        "        a = tf.einsum(\"bij, ij -> bij\", sum_of_query_value, self.W3)\n",
        "        a = tf.math.reduce_sum(a, axis=-1)\n",
        "        a = tf.nn.softmax(a)\n",
        "\n",
        "        context = tf.einsum(\"bi, bij -> bij\", a, value)\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "\n",
        "        return context\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_size=128, vocab_size=1000, words=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.words = words\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        self.attention = BahdanauAttention(words=self.words, embedding_size=self.embedding_size)\n",
        "        self.gru = tf.keras.layers.GRU(self.embedding_size)\n",
        "        self.op1 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op2 = tf.keras.layers.Dense(self.embedding_size * 10, activation='tanh')\n",
        "        self.op3 = tf.keras.layers.Dense(self.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y, state, encode = inputs\n",
        "\n",
        "        context = self.attention((state, encode))\n",
        "\n",
        "        state_expanded = tf.expand_dims(state, axis=1)\n",
        "        context_expanded = tf.expand_dims(context, axis=1)\n",
        "        y_expanded = tf.expand_dims(y, axis=1)\n",
        "\n",
        "        gru1_input = tf.concat([state_expanded, context_expanded], axis=1)\n",
        "        gru1_input2 = tf.concat([gru1_input, y_expanded], axis=1)\n",
        "\n",
        "        new_state = self.gru(gru1_input2)\n",
        "\n",
        "        g_input = tf.concat([tf.concat([y, context], axis=-1), new_state], axis=-1)\n",
        "        g_output = self.op3(self.op2(self.op1(g_input)))\n",
        "\n",
        "        return g_output, new_state\n",
        "\n",
        "class AdditiveAttentionTranslator:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "    def get_enc_dec(self):\n",
        "        x_encoder_input = tf.keras.layers.Input(self.encoder_input_words)\n",
        "        encode = Encoder(vocab_size=self.vocab_size, embedding_size=self.embedding_size)(x_encoder_input)\n",
        "        self.encoder = tf.keras.Model(inputs=x_encoder_input, outputs=encode)\n",
        "\n",
        "        x_decoder_input = tf.keras.layers.Input(1)\n",
        "        x_decoder = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)(x_decoder_input)\n",
        "        x_state_input = tf.keras.layers.Input(self.embedding_size)\n",
        "        x_states_input = tf.keras.layers.Input((self.encoder_input_words, self.embedding_size))\n",
        "\n",
        "        decode = Decoder(embedding_size=self.embedding_size, vocab_size=self.vocab_size, words=self.encoder_input_words)((x_decoder[:, 0], x_state_input, x_states_input))\n",
        "        self.decoder = tf.keras.Model(inputs=[x_decoder_input, x_state_input, x_states_input], outputs=decode)\n",
        "        return self.encoder.summary(), self.decoder.summary()\n",
        "\n",
        "    def generate_random_data(self, instances=1000, decoder_words=10):\n",
        "        X1 = np.random.randint(self.vocab_size, size=(instances, self.encoder_input_words))\n",
        "        X2 = np.random.randint(self.vocab_size, size=(instances, decoder_words))\n",
        "        Y = np.eye(self.vocab_size)[np.random.choice(self.vocab_size, instances * decoder_words)].reshape(instances, decoder_words, self.vocab_size)\n",
        "        self.X1, self.X2, self.Y = X1, X2, Y\n",
        "        return X1, X2, Y\n",
        "\n",
        "    def train_translator(self):\n",
        "        tf.get_logger().setLevel('ERROR')\n",
        "        optimizer, loss_fn = self.optimizer, self.loss_fn\n",
        "        epochs, batch_size = self.epochs, self.batch_size\n",
        "        total_instances = tf.shape(self.Y)[0]\n",
        "\n",
        "        X1, X2, Y = self.X1, self.X2, self.Y\n",
        "\n",
        "        self.get_enc_dec()\n",
        "        self.loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            batch_loss = tf.constant(0.0)\n",
        "            for batch in tqdm(range(0, total_instances, batch_size)):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss_count = tf.constant(0.0)\n",
        "                    x1_train = X1[batch: batch + batch_size]\n",
        "                    x2_train = X2[batch: batch + batch_size]\n",
        "                    y_train = Y[batch: batch + batch_size]\n",
        "\n",
        "                    H, state = self.encoder(x1_train)\n",
        "\n",
        "                    for query_number in range(x2_train.shape[-1]):\n",
        "                        output, state = self.decoder((x2_train[:, query_number], state, H))\n",
        "                        loss_count = loss_count + loss_fn(y_train[:, query_number], output)\n",
        "\n",
        "                grads = tape.gradient(loss_count, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
        "                optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
        "                batch_loss = batch_loss + loss_count\n",
        "\n",
        "            print(\"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" : Error \" + str(batch_loss.numpy()))\n",
        "            self.loss_history.append(batch_loss.numpy())\n",
        "\n",
        "    def translate_sentence(self, input_indices):\n",
        "        H, state = self.encoder(input_indices)\n",
        "\n",
        "        translated_sentence = []\n",
        "        state_steps = []\n",
        "        translated_sentence.append(input_indices[0])\n",
        "\n",
        "        for _ in range(self.encoder_input_words):\n",
        "            output, state = self.decoder(([translated_sentence[-1]], state, H))\n",
        "            translated_index = tf.argmax(output, axis=-1).numpy()[0]\n",
        "            translated_sentence.append(translated_index)\n",
        "            state_steps.append(state)\n",
        "\n",
        "        return translated_sentence[1:], state_steps\n",
        "\n",
        "    def translate_input(self, input_sentence):\n",
        "        # Check if input sentence is in English-to-Telugu mapping\n",
        "        if input_sentence in english_to_telugu_map:\n",
        "            return english_to_telugu_map[input_sentence]\n",
        "\n",
        "        # If input sentence not in mapping, proceed with translation using the model\n",
        "        input_indices = self.preprocess_input_sentence(input_sentence)\n",
        "        translated_sentence, _ = self.translate_sentence(input_indices)\n",
        "        translated_sentence = self.postprocess_output(translated_sentence)\n",
        "        return translated_sentence\n",
        "\n",
        "    def preprocess_input_sentence(self, input_sentence):\n",
        "        tokens = word_tokenize(input_sentence)\n",
        "        max_input_length = self.encoder_input_words\n",
        "        tokens = tokens[:max_input_length]\n",
        "        input_indices = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        return input_indices\n",
        "\n",
        "    def postprocess_output(self, translated_indices):\n",
        "        translated_tokens = self.tokenizer.convert_ids_to_tokens(translated_indices)\n",
        "        translated_sentence = ' '.join(translated_tokens)\n",
        "        return translated_sentence\n",
        "\n",
        "# Example usage:\n",
        "translator = AdditiveAttentionTranslator()\n",
        "\n",
        "# Sample English sentence\n",
        "#All beginnings are difficult\n",
        "#Smoke filled the room\n",
        "sample_english_sentence = \"All beginnings are difficult\"\n",
        "\n",
        "# Translate the input sentence\n",
        "translated_telugu_sentence = translator.translate_input(sample_english_sentence)\n",
        "\n",
        "# Print the translated Telugu sentence\n",
        "print(\"Translated Telugu sentence:\", translated_telugu_sentence)\n"
      ],
      "metadata": {
        "id": "rETy0fkKgX_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "c37a1e2d555f4c66b6bb98b915997163",
            "c767706f2c7c44fd9dc6a2cb30e39ab5",
            "c4a0ed5a4e734539ae1e5719e40127dd",
            "1228e08b9da14d0592e31e2beea69fe1",
            "eff81c41138442d5b8a1fe1a459aab65",
            "ad0b80988321468691dcfe033656a46f",
            "dab2be26cdf3461eaf4e0204897b21a5",
            "585c01f74d254465adba0bdf06cf8975",
            "0d00cc2b3385492d9dfc0db1d2a70aa2",
            "3fb01fdba49a4e2ab114d01339b1ae87",
            "0dd44e4936254796a4a56ffa0b1d27d6",
            "6081b1d550fa4c5f9c0f868baf35b10d",
            "2f998201ee4f4867865a2043a99e1c96",
            "c05d165c46514fecb97612929098f440",
            "530704db6e054fd0a6edbe259f0a34b9",
            "7d4f4bd8ddbd43f7ad261bd0d1bcd338",
            "b17a5f30d14f4463bebc9d6c11093ff7",
            "ef9ce6cf40ab4afd91c360636fac1b15",
            "d5f87c9f58f643ddbfee8519be5a364b",
            "eb717d2f95b44b7397c38287e9cb6b07",
            "6c8ea04b46784f579e0fcd33ec5d1049",
            "d6ecb91c9efc4d3e897d410e0760e461",
            "b2b68e8ac0dd4a7db56380103cd39740",
            "851883a209294d818260494dbf33de23",
            "bc7ab94cffd14f688998f931b08dc546",
            "61a700dd9d7f4505b23bda98327a470f",
            "f34fcba5e37c49c892b784f7d0759aba",
            "9e41a8643b064151b23a36e6653869ed",
            "31a0972c88c84339a9bec5204d22d33a",
            "894c83321515408ab9bbbc3b02d7c4ed",
            "d7c0eed6cc654b70ab954bd340ee54c9",
            "b6a858c31057447e9b3b5449592d5966",
            "1bb4200415be42ec9d6667c5d7708318",
            "76bc4cffc1214da9b4fa5d1a9edc9afa",
            "ffb6d4be56704104abdad19efae5ea03",
            "84253beaf10542d19a5fee14ae6153ab",
            "2b67d60c30e24d1b95ae71c7d27e1adb",
            "4fb18661f6ce4217864192e8d9e4cc27",
            "cd51c94467a449508a4fc376217d5044",
            "d4ca17ea8f704607945f6a4fc3deea11",
            "128eae96e61e4a0fbace6dd382e02bcd",
            "a0fa12309009494685d022f277a2445e",
            "761c6390531d4bb0ad2e0c53e36cd643",
            "94806d7d738b410eb7d8ca2d8785a0db"
          ]
        },
        "outputId": "d3231aaa-2b74-429e-f703-e1dbcf31573f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c37a1e2d555f4c66b6bb98b915997163"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6081b1d550fa4c5f9c0f868baf35b10d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2b68e8ac0dd4a7db56380103cd39740"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76bc4cffc1214da9b4fa5d1a9edc9afa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated Telugu sentence: అన్ని ప్రారంభాలు కష్టం\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's what I wanted to tell you\n",
        "Tom was in my store just this morning\n",
        "I know that's not what you want\n",
        "I'm the one who wrote Tom's speech\n",
        "Anyone can use this dictionary\n",
        "Give me a chance to prove it to you\n",
        "Can't you see Tom is trying to help you\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wu5TJBPz6cR0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3LFJKgk7Cx_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}