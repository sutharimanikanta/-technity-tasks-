{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sutharimanikanta/-technity-tasks-/blob/main/myliver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "id": "jeVjiFCT4tYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1816c627-b876-4a18-bd30-d11e250e8e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.4.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "OM2AUxU-Yn4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model imports"
      ],
      "metadata": {
        "id": "jPP_9skvfZIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real\n",
        "import itertools\n",
        "from sklearn.utils import resample\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/Indian Liver Patient Dataset (ILPD).csv')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "data['gender'] = label_encoder.fit_transform(data['gender'])\n",
        "X = data.drop(columns=['is_patient'])\n",
        "y = data['is_patient']\n",
        "# Count the number of positive and negative cases\n",
        "num_positive_cases = (y == 1).sum()\n",
        "num_negative_cases = (y == 0).sum()"
      ],
      "metadata": {
        "id": "kpJxF4kMfYbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the desired number of samples for each class\n",
        "desired_num_samples = max(num_positive_cases, num_negative_cases)\n",
        "\n",
        "import itertools\n",
        "\n",
        "# Define all possible methods and their combinations\n",
        "methods = ['L1', 'L2', 'Bayesian']  # Adjust as needed\n",
        "all_combinations = []\n",
        "\n",
        "# Generate all subsets and permutations of methods\n",
        "for r in range(1, len(methods) + 1):\n",
        "    for subset in itertools.combinations(methods, r):\n",
        "        all_combinations.append(subset)\n",
        "\n",
        "# Add reverse combinations\n",
        "for combination in all_combinations.copy():\n",
        "    if len(combination) > 1:\n",
        "        for r in range(1, len(combination)):\n",
        "            for reverse_subset in itertools.combinations(combination, r):\n",
        "                if reverse_subset not in all_combinations:\n",
        "                    all_combinations.append(reverse_subset)\n"
      ],
      "metadata": {
        "id": "1vU4OuMvgFtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(X, y):\n",
        "    # Resample the data to balance the classes\n",
        "    ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "    # Change class label 2 to 0 in y_resampled (assuming binary classification)\n",
        "    y_resampled[y_resampled == 2] = 0\n",
        "\n",
        "    # Creating a new DataFrame with the resampled data\n",
        "    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "    resampled_data['is_patient'] = y_resampled\n",
        "\n",
        "    # Impute missing values with the median\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    data_imputed = pd.DataFrame(imputer.fit_transform(resampled_data), columns=resampled_data.columns)\n",
        "\n",
        "    # Detect and handle outliers\n",
        "    z_scores = np.abs((data_imputed - data_imputed.mean()) / data_imputed.std())\n",
        "    outliers = (z_scores > 3)\n",
        "    for column in data_imputed.columns:\n",
        "        median = data_imputed[column].median()\n",
        "        data_imputed.loc[outliers[column], column] = median\n",
        "\n",
        "    # Remove the 'is_patient' column before scaling\n",
        "    y_resampled = data_imputed['is_patient']\n",
        "    data_no_outliers = data_imputed.drop(columns=['is_patient'])\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data_no_outliers), columns=data_no_outliers.columns)\n",
        "    fa = FactorAnalysis(n_components=10)  # Adjust as needed\n",
        "    X_fa = fa.fit_transform(data_scaled)\n",
        "\n",
        "# Apply Linear Discriminant Analysis (LDA)\n",
        "    lda = LDA(n_components=1)\n",
        "    X_lda = lda.fit_transform(data_scaled, y_resampled)\n",
        "    # Convert the FA and LDA results to DataFrames\n",
        "    df_fa = pd.DataFrame(X_fa, columns=[f'FA_Component_{i+1}' for i in range(X_fa.shape[1])])\n",
        "    df_lda = pd.DataFrame(X_lda, columns=['LDA_Component'])\n",
        "\n",
        "    # Integrate the datasets by concatenating them along the columns\n",
        "    final_data = pd.concat([df_fa, df_lda], axis=1)\n",
        "\n",
        "    return final_data, y_resampled\n",
        "final_data, y_resampled=preprocess_data(X, y)"
      ],
      "metadata": {
        "id": "NRnSAuYege1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pymc3"
      ],
      "metadata": {
        "id": "-2R610yixH--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import arviz as az\n",
        "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "rBCA7zzYw8Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge, Lasso\n",
        "import numpy as np\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "# FGSM function for adversarial examples\n",
        "def fgsm_attack(model, criterion, data, target, epsilon):\n",
        "    data.requires_grad = True\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    data_grad = data.grad.data\n",
        "    sign_data_grad = data_grad.sign()\n",
        "    perturbed_data = data + epsilon * sign_data_grad\n",
        "    return perturbed_data\n",
        "\n",
        "def perform_l2_regularization(final_data, y_resampled):\n",
        "    ridge = Ridge(alpha=0.1)  # You can adjust the alpha value as needed\n",
        "    ridge.fit(final_data, y_resampled)\n",
        "    selected_features_indices = np.where(ridge.coef_ != 0)[0]\n",
        "    selected_features_names = final_data.columns[selected_features_indices].tolist()\n",
        "    return selected_features_names\n",
        "\n",
        "def perform_l1_regularization(final_data, y_resampled):\n",
        "    lasso = Lasso(alpha=0.1)\n",
        "    lasso.fit(final_data, y_resampled)\n",
        "    selected_features_indices = np.where(lasso.coef_ != 0)[0]\n",
        "    selected_features_names = final_data.columns[selected_features_indices].tolist()\n",
        "    return selected_features_names\n",
        "\n",
        "def perform_bayesian_optimization(final_data, y_resampled):\n",
        "    n_features = final_data.shape[1]\n",
        "    search_space = [Integer(0, 1) for _ in range(n_features)] + [Real(1e-6, 1e+6, prior='log-uniform')]\n",
        "\n",
        "    def objective(params):\n",
        "        selected_features, C = params[:-1], params[-1]\n",
        "        selected_features = np.array(selected_features, dtype=bool)\n",
        "        X_selected = final_data.iloc[:, selected_features]\n",
        "        if X_selected.shape[1] == 0:\n",
        "            X_selected = final_data\n",
        "        model = LogisticRegression(C=C, penalty='l2', solver='liblinear', random_state=42)\n",
        "        scores = cross_val_score(model, X_selected, y_resampled, cv=5, scoring='accuracy')\n",
        "        return -np.mean(scores)\n",
        "\n",
        "    result = gp_minimize(\n",
        "        func=objective,\n",
        "        dimensions=search_space,\n",
        "        n_calls=50,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    best_features = np.array(result.x[:-1], dtype=bool)\n",
        "    best_C = result.x[-1]\n",
        "    selected_features_indices = np.arange(n_features)[best_features]\n",
        "    selected_features_names = final_data.columns[selected_features_indices].tolist()\n",
        "\n",
        "    return selected_features_names\n",
        "\n",
        "def select_features(selected_methods, final_data, y_resampled):\n",
        "    selected_features_names = []\n",
        "\n",
        "    if 'L1' in selected_methods:\n",
        "        l1_selected_features = perform_l1_regularization(final_data, y_resampled)\n",
        "        selected_features_names.extend(l1_selected_features)\n",
        "        print(f\"Features selected by L1: {l1_selected_features}\")\n",
        "\n",
        "    if 'L2' in selected_methods:\n",
        "        l2_selected_features = perform_l2_regularization(final_data, y_resampled)\n",
        "        selected_features_names.extend(l2_selected_features)\n",
        "        print(f\"Features selected by L2: {l2_selected_features}\")\n",
        "\n",
        "    if 'Bayesian' in selected_methods:\n",
        "        bayesian_selected_features = perform_bayesian_optimization(final_data, y_resampled)\n",
        "        selected_features_names.extend(bayesian_selected_features)\n",
        "        print(f\"Features selected by Bayesian: {bayesian_selected_features}\")\n",
        "\n",
        "    return selected_features_names\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, epsilon=0.1):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Number of epochs can be adjusted\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train_tensor)\n",
        "        loss = criterion(output, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        adv_data = fgsm_attack(model, criterion, X_train_tensor, y_train_tensor, epsilon)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        adv_output = model(adv_data)\n",
        "        adv_loss = criterion(adv_output, y_train_tensor)\n",
        "        adv_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test_tensor)\n",
        "        y_pred = output.argmax(dim=1).numpy()\n",
        "        y_proba = nn.Softmax(dim=1)(output).numpy()[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n"
      ],
      "metadata": {
        "id": "Cj5XDCRhx88I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Koa3g4X7_WzL",
        "outputId": "0f734059-1416-4700-f02b-405714877217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lazypredict\n",
            "  Downloading lazypredict-0.2.12-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lazypredict) (8.1.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.66.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from lazypredict) (1.4.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (from lazypredict) (4.1.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from lazypredict) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm->lazypredict) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->lazypredict) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lazypredict) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->lazypredict) (1.16.0)\n",
            "Installing collected packages: lazypredict\n",
            "Successfully installed lazypredict-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lazypredict\n",
        "from lazypredict.Supervised import LazyRegressor"
      ],
      "metadata": {
        "id": "oSrcrWB2_UKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate neural network model performance with adversarial training\n",
        "def evaluate_nn_model(model, X_train, X_test, y_train, y_test, epsilon=0.1):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(5):  # Number of epochs can be adjusted\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_train_tensor)\n",
        "        loss = criterion(output, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        adv_data = fgsm_attack(model, criterion, X_train_tensor, y_train_tensor, epsilon)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        adv_output = model(adv_data)\n",
        "        adv_loss = criterion(adv_output, y_train_tensor)\n",
        "        adv_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test_tensor)\n",
        "        y_pred = output.argmax(dim=1).numpy()\n",
        "        y_proba = nn.Softmax(dim=1)(output).numpy()[:, 1]\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Evaluate non-neural network models\n",
        "def evaluate_sklearn_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else np.zeros_like(y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Run pipeline with adversarial training for neural networks and standard evaluation for others\n",
        "def run_pipeline(selected_methods, final_data, y_resampled, epsilon=0.1):\n",
        "    selected_features_names = select_features(selected_methods, final_data, y_resampled)\n",
        "    X_selected = final_data[selected_features_names] if selected_features_names else final_data.copy()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "    classifiers = {\n",
        "        \"Logistic Regression\": LogisticRegression(solver='liblinear', random_state=42),\n",
        "        \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM\": SVC(probability=True, random_state=42),\n",
        "        \"MLP\": MLPClassifier(random_state=42)\n",
        "    }\n",
        "\n",
        "    param_grids = {\n",
        "        \"Logistic Regression\": {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        },\n",
        "        \"K-Nearest Neighbors\": {\n",
        "            'n_neighbors': [3, 5, 7],\n",
        "            'weights': ['uniform', 'distance']\n",
        "        },\n",
        "        \"Random Forest\": {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 5, 10, 15]\n",
        "        },\n",
        "        \"SVM\": {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf']\n",
        "        },\n",
        "        \"MLP\": {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "            'alpha': [0.0001, 0.001, 0.01]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        param_grid = param_grids.get(name)\n",
        "        if param_grid:\n",
        "            grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_params = grid_search.best_params_\n",
        "            classifiers[name] = clf.set_params(**best_params)\n",
        "            if name == \"MLP\":\n",
        "                # Convert MLPClassifier to a PyTorch model for adversarial training\n",
        "                class PyTorchMLP(nn.Module):\n",
        "                    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
        "                        super(PyTorchMLP, self).__init__()\n",
        "                        self.hidden_layers = nn.ModuleList()\n",
        "                        self.hidden_layers.append(nn.Linear(input_size, hidden_layer_sizes[0]))\n",
        "                        for i in range(1, len(hidden_layer_sizes)):\n",
        "                            self.hidden_layers.append(nn.Linear(hidden_layer_sizes[i-1], hidden_layer_sizes[i]))\n",
        "                        self.output_layer = nn.Linear(hidden_layer_sizes[-1], output_size)\n",
        "                        self.activation = nn.ReLU()\n",
        "\n",
        "                    def forward(self, x):\n",
        "                        for layer in self.hidden_layers:\n",
        "                            x = self.activation(layer(x))\n",
        "                        x = self.output_layer(x)\n",
        "                        return x\n",
        "\n",
        "                input_size = X_train.shape[1]\n",
        "                hidden_layer_sizes = best_params['hidden_layer_sizes']\n",
        "                output_size = len(np.unique(y_train))\n",
        "                pytorch_mlp = PyTorchMLP(input_size, hidden_layer_sizes, output_size)\n",
        "                results = evaluate_nn_model(pytorch_mlp, X_train, X_test, y_train, y_test, epsilon)\n",
        "            else:\n",
        "                results = evaluate_sklearn_model(clf, X_train, X_test, y_train, y_test)\n",
        "            print(f\"{name} after {selected_methods}: Accuracy={results[0]:.2f}, Precision={results[1]:.2f}, Recall={results[2]:.2f}, F1-Score={results[3]:.2f}, AUC={results[4]:.2f}\")\n",
        "\n",
        "    # Use LazyRegressor for regression tasks\n",
        "    lazy_regressor = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None, random_state=42, regressors='all')\n",
        "    models, predictions = lazy_regressor.fit(X_train, X_test, y_train, y_test)\n",
        "    print(models)\n",
        "\n",
        "    return classifiers\n",
        "\n",
        "# Preprocess the data\n",
        "# [Include your existing preprocess_data function here]\n",
        "\n",
        "# All combinations of feature selection methods\n",
        "methods = ['L1', 'L2', 'Bayesian']\n",
        "all_combinations = []\n",
        "for r in range(1, len(methods) + 1):\n",
        "    all_combinations.extend(itertools.combinations(methods, r))\n",
        "\n",
        "# Perform pipeline for each combination of methods\n",
        "results = []\n",
        "for combination in all_combinations:\n",
        "    print(f\"\\nRunning pipeline for methods: {combination}\")\n",
        "    classifiers = run_pipeline(combination, final_data, y_resampled)\n",
        "    results.append((combination, classifiers))\n",
        "\n",
        "# Further process or display results as needed\n",
        "for combination, classifiers in results:\n",
        "    print(\"\\nFinal Model Performances for combination:\", combination)\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"{name}: {clf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1Grv6OeAk5f",
        "outputId": "bb442143-083d-4254-ff6b-9205dde02635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running pipeline for methods: ('L1',)\n",
            "Features selected by L1: ['LDA_Component']\n",
            "Logistic Regression after ('L1',): Accuracy=0.76, Precision=0.83, Recall=0.59, F1-Score=0.69, AUC=0.79\n",
            "K-Nearest Neighbors after ('L1',): Accuracy=0.83, Precision=0.90, Recall=0.69, F1-Score=0.78, AUC=0.94\n",
            "Random Forest after ('L1',): Accuracy=0.83, Precision=0.84, Recall=0.76, F1-Score=0.80, AUC=0.88\n",
            "SVM after ('L1',): Accuracy=0.74, Precision=0.86, Recall=0.51, F1-Score=0.64, AUC=0.79\n",
            "MLP after ('L1',): Accuracy=0.74, Precision=0.88, Recall=0.49, F1-Score=0.63, AUC=0.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:02<00:00, 15.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 221\n",
            "[LightGBM] [Info] Number of data points in the train set: 665, number of used features: 1\n",
            "[LightGBM] [Info] Start training from score 0.512782\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n",
            "Model                                                                         \n",
            "ExtraTreesRegressor                          0.43       0.44  0.37        0.10\n",
            "RandomForestRegressor                        0.41       0.41  0.38        0.23\n",
            "GradientBoostingRegressor                    0.40       0.40  0.39        0.16\n",
            "ExtraTreeRegressor                           0.39       0.39  0.39        0.01\n",
            "BaggingRegressor                             0.39       0.39  0.39        0.03\n",
            "LGBMRegressor                                0.37       0.38  0.39        0.15\n",
            "HistGradientBoostingRegressor                0.36       0.36  0.40        0.64\n",
            "KNeighborsRegressor                          0.33       0.33  0.41        0.02\n",
            "XGBRegressor                                 0.30       0.31  0.41        0.09\n",
            "DecisionTreeRegressor                        0.29       0.30  0.42        0.01\n",
            "AdaBoostRegressor                            0.27       0.28  0.42        0.03\n",
            "GaussianProcessRegressor                     0.25       0.26  0.43        0.08\n",
            "MLPRegressor                                 0.25       0.25  0.43        0.41\n",
            "HuberRegressor                               0.23       0.24  0.43        0.02\n",
            "SGDRegressor                                 0.22       0.23  0.44        0.02\n",
            "TransformedTargetRegressor                   0.22       0.23  0.44        0.01\n",
            "OrthogonalMatchingPursuit                    0.22       0.23  0.44        0.02\n",
            "LinearRegression                             0.22       0.23  0.44        0.01\n",
            "LassoLarsCV                                  0.22       0.23  0.44        0.01\n",
            "LassoLarsIC                                  0.22       0.23  0.44        0.01\n",
            "LarsCV                                       0.22       0.23  0.44        0.04\n",
            "Lars                                         0.22       0.23  0.44        0.03\n",
            "Ridge                                        0.22       0.23  0.44        0.01\n",
            "SVR                                          0.22       0.23  0.44        0.05\n",
            "BayesianRidge                                0.22       0.23  0.44        0.01\n",
            "RidgeCV                                      0.22       0.23  0.44        0.01\n",
            "LassoCV                                      0.22       0.23  0.44        0.06\n",
            "ElasticNetCV                                 0.22       0.23  0.44        0.06\n",
            "PassiveAggressiveRegressor                   0.19       0.20  0.45        0.01\n",
            "LinearSVR                                    0.17       0.18  0.45        0.01\n",
            "TweedieRegressor                             0.14       0.14  0.46        0.02\n",
            "PoissonRegressor                             0.09       0.10  0.47        0.02\n",
            "NuSVR                                       -0.02      -0.01  0.50        0.07\n",
            "DummyRegressor                              -0.02      -0.02  0.50        0.01\n",
            "ElasticNet                                  -0.02      -0.02  0.50        0.01\n",
            "Lasso                                       -0.02      -0.02  0.50        0.03\n",
            "LassoLars                                   -0.02      -0.02  0.50        0.01\n",
            "KernelRidge                                 -0.68      -0.67  0.64        0.04\n",
            "RANSACRegressor                             -1.24      -1.23  0.74        0.03\n",
            "\n",
            "Running pipeline for methods: ('L2',)\n",
            "Features selected by L2: ['FA_Component_1', 'FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'LDA_Component']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression after ('L2',): Accuracy=0.75, Precision=0.80, Recall=0.59, F1-Score=0.68, AUC=0.79\n",
            "K-Nearest Neighbors after ('L2',): Accuracy=0.80, Precision=0.89, Recall=0.64, F1-Score=0.74, AUC=0.94\n",
            "Random Forest after ('L2',): Accuracy=0.85, Precision=0.93, Recall=0.72, F1-Score=0.81, AUC=0.96\n",
            "SVM after ('L2',): Accuracy=0.71, Precision=0.81, Recall=0.45, F1-Score=0.58, AUC=0.78\n",
            "MLP after ('L2',): Accuracy=0.38, Precision=0.40, Recall=0.72, F1-Score=0.51, AUC=0.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:02<00:00, 16.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1327\n",
            "[LightGBM] [Info] Number of data points in the train set: 665, number of used features: 6\n",
            "[LightGBM] [Info] Start training from score 0.512782\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n",
            "Model                                                                         \n",
            "ExtraTreesRegressor                          0.68       0.69  0.27        0.14\n",
            "HistGradientBoostingRegressor                0.67       0.68  0.28        0.22\n",
            "LGBMRegressor                                0.67       0.68  0.28        0.10\n",
            "XGBRegressor                                 0.65       0.66  0.29        0.12\n",
            "BaggingRegressor                             0.65       0.66  0.29        0.05\n",
            "RandomForestRegressor                        0.64       0.66  0.29        0.27\n",
            "GradientBoostingRegressor                    0.53       0.54  0.34        0.23\n",
            "MLPRegressor                                 0.35       0.37  0.39        0.61\n",
            "KNeighborsRegressor                          0.27       0.30  0.42        0.01\n",
            "DecisionTreeRegressor                        0.27       0.30  0.42        0.01\n",
            "SVR                                          0.25       0.27  0.42        0.03\n",
            "AdaBoostRegressor                            0.22       0.24  0.43        0.05\n",
            "OrthogonalMatchingPursuit                    0.20       0.23  0.44        0.01\n",
            "OrthogonalMatchingPursuitCV                  0.20       0.23  0.44        0.02\n",
            "HuberRegressor                               0.20       0.23  0.44        0.01\n",
            "LassoLarsIC                                  0.19       0.22  0.44        0.01\n",
            "TransformedTargetRegressor                   0.19       0.22  0.44        0.01\n",
            "LinearRegression                             0.19       0.22  0.44        0.01\n",
            "Lars                                         0.19       0.22  0.44        0.01\n",
            "Ridge                                        0.19       0.22  0.44        0.01\n",
            "SGDRegressor                                 0.19       0.22  0.44        0.01\n",
            "RidgeCV                                      0.19       0.22  0.44        0.01\n",
            "LarsCV                                       0.19       0.22  0.44        0.02\n",
            "LassoLarsCV                                  0.19       0.22  0.44        0.01\n",
            "LassoCV                                      0.19       0.22  0.44        0.06\n",
            "BayesianRidge                                0.18       0.21  0.44        0.01\n",
            "ElasticNetCV                                 0.18       0.21  0.44        0.05\n",
            "ExtraTreeRegressor                           0.15       0.18  0.45        0.01\n",
            "TweedieRegressor                             0.14       0.17  0.45        0.01\n",
            "PoissonRegressor                             0.10       0.13  0.46        0.02\n",
            "NuSVR                                        0.06       0.09  0.47        0.07\n",
            "LinearSVR                                    0.02       0.05  0.48        0.01\n",
            "ElasticNet                                  -0.05      -0.02  0.50        0.01\n",
            "LassoLars                                   -0.05      -0.02  0.50        0.01\n",
            "DummyRegressor                              -0.05      -0.02  0.50        0.01\n",
            "Lasso                                       -0.05      -0.02  0.50        0.01\n",
            "KernelRidge                                 -0.74      -0.67  0.64        0.02\n",
            "RANSACRegressor                             -0.88      -0.82  0.67        0.11\n",
            "PassiveAggressiveRegressor                  -1.48      -1.39  0.77        0.01\n",
            "GaussianProcessRegressor                    -6.25      -5.99  1.31        0.05\n",
            "\n",
            "Running pipeline for methods: ('Bayesian',)\n",
            "Features selected by Bayesian: ['FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'FA_Component_6', 'FA_Component_7', 'LDA_Component']\n",
            "Logistic Regression after ('Bayesian',): Accuracy=0.75, Precision=0.80, Recall=0.59, F1-Score=0.68, AUC=0.79\n",
            "K-Nearest Neighbors after ('Bayesian',): Accuracy=0.85, Precision=0.93, Recall=0.72, F1-Score=0.81, AUC=0.92\n",
            "Random Forest after ('Bayesian',): Accuracy=0.87, Precision=0.91, Recall=0.79, F1-Score=0.84, AUC=0.96\n",
            "SVM after ('Bayesian',): Accuracy=0.74, Precision=0.86, Recall=0.51, F1-Score=0.64, AUC=0.79\n",
            "MLP after ('Bayesian',): Accuracy=0.55, Precision=0.00, Recall=0.00, F1-Score=0.00, AUC=0.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:02<00:00, 17.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1105\n",
            "[LightGBM] [Info] Number of data points in the train set: 665, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 0.512782\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n",
            "Model                                                                         \n",
            "ExtraTreesRegressor                          0.67       0.68  0.28        0.13\n",
            "LGBMRegressor                                0.66       0.67  0.28        0.09\n",
            "HistGradientBoostingRegressor                0.66       0.67  0.28        0.25\n",
            "XGBRegressor                                 0.64       0.66  0.29        0.10\n",
            "RandomForestRegressor                        0.61       0.63  0.30        0.25\n",
            "BaggingRegressor                             0.53       0.55  0.33        0.03\n",
            "GradientBoostingRegressor                    0.48       0.51  0.35        0.17\n",
            "DecisionTreeRegressor                        0.34       0.37  0.39        0.01\n",
            "MLPRegressor                                 0.26       0.30  0.42        0.57\n",
            "SVR                                          0.24       0.28  0.42        0.03\n",
            "KNeighborsRegressor                          0.23       0.26  0.43        0.01\n",
            "OrthogonalMatchingPursuitCV                  0.19       0.23  0.44        0.02\n",
            "OrthogonalMatchingPursuit                    0.19       0.23  0.44        0.01\n",
            "HuberRegressor                               0.19       0.23  0.44        0.01\n",
            "ExtraTreeRegressor                           0.19       0.23  0.44        0.01\n",
            "LassoLarsIC                                  0.19       0.22  0.44        0.01\n",
            "Lars                                         0.18       0.22  0.44        0.01\n",
            "LinearRegression                             0.18       0.22  0.44        0.01\n",
            "TransformedTargetRegressor                   0.18       0.22  0.44        0.01\n",
            "Ridge                                        0.18       0.22  0.44        0.01\n",
            "SGDRegressor                                 0.18       0.22  0.44        0.01\n",
            "RidgeCV                                      0.18       0.22  0.44        0.01\n",
            "LarsCV                                       0.18       0.22  0.44        0.02\n",
            "LassoLarsCV                                  0.18       0.22  0.44        0.01\n",
            "LassoCV                                      0.18       0.22  0.44        0.07\n",
            "BayesianRidge                                0.18       0.21  0.44        0.01\n",
            "ElasticNetCV                                 0.18       0.21  0.44        0.05\n",
            "AdaBoostRegressor                            0.18       0.21  0.44        0.08\n",
            "TweedieRegressor                             0.10       0.14  0.46        0.01\n",
            "PoissonRegressor                             0.06       0.10  0.47        0.01\n",
            "LinearSVR                                    0.05       0.09  0.48        0.01\n",
            "NuSVR                                        0.03       0.07  0.48        0.07\n",
            "Lasso                                       -0.06      -0.02  0.50        0.01\n",
            "ElasticNet                                  -0.06      -0.02  0.50        0.01\n",
            "DummyRegressor                              -0.06      -0.02  0.50        0.01\n",
            "LassoLars                                   -0.06      -0.02  0.50        0.01\n",
            "KernelRidge                                 -0.75      -0.67  0.64        0.02\n",
            "PassiveAggressiveRegressor                  -1.94      -1.81  0.83        0.01\n",
            "GaussianProcessRegressor                   -45.30     -43.35  3.31        0.05\n",
            "\n",
            "Running pipeline for methods: ('L1', 'L2')\n",
            "Features selected by L1: ['LDA_Component']\n",
            "Features selected by L2: ['FA_Component_1', 'FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'LDA_Component']\n",
            "Logistic Regression after ('L1', 'L2'): Accuracy=0.75, Precision=0.80, Recall=0.59, F1-Score=0.68, AUC=0.79\n",
            "K-Nearest Neighbors after ('L1', 'L2'): Accuracy=0.79, Precision=0.88, Recall=0.61, F1-Score=0.72, AUC=0.94\n",
            "Random Forest after ('L1', 'L2'): Accuracy=0.86, Precision=0.91, Recall=0.77, F1-Score=0.83, AUC=0.96\n",
            "SVM after ('L1', 'L2'): Accuracy=0.76, Precision=0.91, Recall=0.52, F1-Score=0.66, AUC=0.81\n",
            "MLP after ('L1', 'L2'): Accuracy=0.44, Precision=0.44, Recall=0.89, F1-Score=0.59, AUC=0.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 2634.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Adjusted R-Squared, R-Squared, RMSE, Time Taken]\n",
            "Index: []\n",
            "\n",
            "Running pipeline for methods: ('L1', 'Bayesian')\n",
            "Features selected by L1: ['LDA_Component']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features selected by Bayesian: ['FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'FA_Component_6', 'FA_Component_7', 'LDA_Component']\n",
            "Logistic Regression after ('L1', 'Bayesian'): Accuracy=0.75, Precision=0.80, Recall=0.59, F1-Score=0.68, AUC=0.79\n",
            "K-Nearest Neighbors after ('L1', 'Bayesian'): Accuracy=0.83, Precision=0.90, Recall=0.71, F1-Score=0.79, AUC=0.95\n",
            "Random Forest after ('L1', 'Bayesian'): Accuracy=0.84, Precision=0.88, Recall=0.76, F1-Score=0.81, AUC=0.96\n",
            "SVM after ('L1', 'Bayesian'): Accuracy=0.72, Precision=0.85, Recall=0.45, F1-Score=0.59, AUC=0.78\n",
            "MLP after ('L1', 'Bayesian'): Accuracy=0.45, Precision=0.45, Recall=1.00, F1-Score=0.62, AUC=0.74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 1404.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Adjusted R-Squared, R-Squared, RMSE, Time Taken]\n",
            "Index: []\n",
            "\n",
            "Running pipeline for methods: ('L2', 'Bayesian')\n",
            "Features selected by L2: ['FA_Component_1', 'FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'LDA_Component']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features selected by Bayesian: ['FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'FA_Component_6', 'FA_Component_7', 'LDA_Component']\n",
            "Logistic Regression after ('L2', 'Bayesian'): Accuracy=0.75, Precision=0.80, Recall=0.59, F1-Score=0.68, AUC=0.79\n",
            "K-Nearest Neighbors after ('L2', 'Bayesian'): Accuracy=0.80, Precision=0.89, Recall=0.64, F1-Score=0.74, AUC=0.93\n",
            "Random Forest after ('L2', 'Bayesian'): Accuracy=0.89, Precision=0.94, Recall=0.80, F1-Score=0.86, AUC=0.97\n",
            "SVM after ('L2', 'Bayesian'): Accuracy=0.74, Precision=0.85, Recall=0.52, F1-Score=0.64, AUC=0.79\n",
            "MLP after ('L2', 'Bayesian'): Accuracy=0.45, Precision=0.45, Recall=1.00, F1-Score=0.62, AUC=0.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 2562.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Adjusted R-Squared, R-Squared, RMSE, Time Taken]\n",
            "Index: []\n",
            "\n",
            "Running pipeline for methods: ('L1', 'L2', 'Bayesian')\n",
            "Features selected by L1: ['LDA_Component']\n",
            "Features selected by L2: ['FA_Component_1', 'FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'LDA_Component']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features selected by Bayesian: ['FA_Component_2', 'FA_Component_3', 'FA_Component_4', 'FA_Component_5', 'FA_Component_6', 'FA_Component_7', 'LDA_Component']\n",
            "Logistic Regression after ('L1', 'L2', 'Bayesian'): Accuracy=0.75, Precision=0.80, Recall=0.59, F1-Score=0.68, AUC=0.79\n",
            "K-Nearest Neighbors after ('L1', 'L2', 'Bayesian'): Accuracy=0.80, Precision=0.89, Recall=0.64, F1-Score=0.74, AUC=0.94\n",
            "Random Forest after ('L1', 'L2', 'Bayesian'): Accuracy=0.89, Precision=0.91, Recall=0.83, F1-Score=0.87, AUC=0.96\n",
            "SVM after ('L1', 'L2', 'Bayesian'): Accuracy=0.74, Precision=0.85, Recall=0.52, F1-Score=0.64, AUC=0.79\n",
            "MLP after ('L1', 'L2', 'Bayesian'): Accuracy=0.45, Precision=0.43, Recall=0.75, F1-Score=0.55, AUC=0.39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 2789.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Adjusted R-Squared, R-Squared, RMSE, Time Taken]\n",
            "Index: []\n",
            "\n",
            "Final Model Performances for combination: ('L1',)\n",
            "Logistic Regression: LogisticRegression(C=0.001, random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(weights='distance')\n",
            "Random Forest: RandomForestClassifier(max_depth=15, n_estimators=200, random_state=42)\n",
            "SVM: SVC(C=0.1, kernel='linear', probability=True, random_state=42)\n",
            "MLP: MLPClassifier(hidden_layer_sizes=(50,), random_state=42)\n",
            "\n",
            "Final Model Performances for combination: ('L2',)\n",
            "Logistic Regression: LogisticRegression(C=0.1, penalty='l1', random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(weights='distance')\n",
            "Random Forest: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n",
            "SVM: SVC(C=0.1, kernel='linear', probability=True, random_state=42)\n",
            "MLP: MLPClassifier(alpha=0.01, hidden_layer_sizes=(50, 50), random_state=42)\n",
            "\n",
            "Final Model Performances for combination: ('Bayesian',)\n",
            "Logistic Regression: LogisticRegression(C=0.1, penalty='l1', random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
            "Random Forest: RandomForestClassifier(max_depth=15, n_estimators=200, random_state=42)\n",
            "SVM: SVC(C=0.1, probability=True, random_state=42)\n",
            "MLP: MLPClassifier(alpha=0.001, hidden_layer_sizes=(50, 50), random_state=42)\n",
            "\n",
            "Final Model Performances for combination: ('L1', 'L2')\n",
            "Logistic Regression: LogisticRegression(C=0.1, penalty='l1', random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(weights='distance')\n",
            "Random Forest: RandomForestClassifier(n_estimators=300, random_state=42)\n",
            "SVM: SVC(C=1, probability=True, random_state=42)\n",
            "MLP: MLPClassifier(random_state=42)\n",
            "\n",
            "Final Model Performances for combination: ('L1', 'Bayesian')\n",
            "Logistic Regression: LogisticRegression(C=0.1, penalty='l1', random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(weights='distance')\n",
            "Random Forest: RandomForestClassifier(n_estimators=200, random_state=42)\n",
            "SVM: SVC(C=0.1, kernel='linear', probability=True, random_state=42)\n",
            "MLP: MLPClassifier(alpha=0.01, hidden_layer_sizes=(50, 50), random_state=42)\n",
            "\n",
            "Final Model Performances for combination: ('L2', 'Bayesian')\n",
            "Logistic Regression: LogisticRegression(C=0.1, penalty='l1', random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(weights='distance')\n",
            "Random Forest: RandomForestClassifier(n_estimators=200, random_state=42)\n",
            "SVM: SVC(C=0.1, probability=True, random_state=42)\n",
            "MLP: MLPClassifier(alpha=0.001, hidden_layer_sizes=(50, 50), random_state=42)\n",
            "\n",
            "Final Model Performances for combination: ('L1', 'L2', 'Bayesian')\n",
            "Logistic Regression: LogisticRegression(C=0.1, penalty='l1', random_state=42, solver='liblinear')\n",
            "K-Nearest Neighbors: KNeighborsClassifier(weights='distance')\n",
            "Random Forest: RandomForestClassifier(n_estimators=300, random_state=42)\n",
            "SVM: SVC(C=0.1, probability=True, random_state=42)\n",
            "MLP: MLPClassifier(alpha=0.001, hidden_layer_sizes=(50, 50), random_state=42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline(selected_methods, final_data, y_resampled):\n",
        "    selected_features_names = select_features(selected_methods, final_data , y_resampled)\n",
        "    X_selected = final_data[selected_features_names] if selected_features_names else final_data.copy()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "    classifiers = {\n",
        "        \"Logistic Regression\": LogisticRegression(solver='liblinear', random_state=42),\n",
        "        \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        \"SVM\": SVC(probability=True, random_state=42),\n",
        "        \"MLP\": MLPClassifier(random_state=42)\n",
        "    }\n",
        "\n",
        "    param_grids = {\n",
        "        \"Logistic Regression\": {\n",
        "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            'penalty': ['l1', 'l2']\n",
        "        },\n",
        "        \"K-Nearest Neighbors\": {\n",
        "            'n_neighbors': [3, 5, 7],\n",
        "            'weights': ['uniform', 'distance']\n",
        "        },\n",
        "        \"Random Forest\": {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 5, 10, 15]\n",
        "        },\n",
        "        \"SVM\": {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'kernel': ['linear', 'rbf']\n",
        "        },\n",
        "        \"MLP\": {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "            'alpha': [0.0001, 0.001, 0.01]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for name, clf in classifiers.items():\n",
        "        param_grid = param_grids.get(name)\n",
        "        if param_grid:\n",
        "            grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            best_params = grid_search.best_params_\n",
        "            classifiers[name] = clf.set_params(**best_params)\n",
        "            results = evaluate_model(clf, X_train, X_test, y_train, y_test)\n",
        "            print(f\"{name} after {selected_methods}: Accuracy={results[0]:.2f}, Precision={results[1]:.2f}, Recall={results[2]:.2f}, F1-Score={results[3]:.2f}, AUC={results[4]:.2f}\")\n",
        "\n",
        "    # Use LazyRegressor for regression tasks\n",
        "    lazy_regressor = LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None, random_state=42, regressors='all')\n",
        "    models, predictions = lazy_regressor.fit(X_train, X_test, y_train, y_test)\n",
        "    print(models)\n",
        "\n",
        "    return classifiers\n",
        "\n",
        "\n",
        "# Preprocess the data\n",
        "\n",
        "# All combinations of feature selection methods\n",
        "methods = ['L1', 'L2', 'Bayesian']\n",
        "all_combinations = []\n",
        "for r in range(1, len(methods) + 1):\n",
        "    all_combinations.extend(itertools.combinations(methods, r))\n",
        "\n",
        "# Perform pipeline for each combination of methods\n",
        "\n",
        "results = []\n",
        "for combination in all_combinations:\n",
        "    print(f\"\\nRunning pipeline for methods: {combination}\")\n",
        "    classifiers = run_pipeline(combination, final_data , y_resampled)\n",
        "    results.append((combination, classifiers))\n",
        "\n",
        "# Further process or display results as needed\n",
        "for combination, classifiers in results:\n",
        "    print(\"\\nFinal Model Performances for combination:\", combination)\n",
        "    for name, clf in classifiers.items():\n",
        "        print(f\"{name}: {clf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "JIdohjoHbEil",
        "outputId": "093d3ec9-dc73-4736-876f-940d3c2f9618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running pipeline for methods: ('L1',)\n",
            "Features selected by L1: ['LDA_Component']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LogisticRegression' object has no attribute 'parameters'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cddda6c7384d>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcombination\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_combinations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nRunning pipeline for methods: {combination}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-cddda6c7384d>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(selected_methods, final_data, y_resampled)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name} after {selected_methods}: Accuracy={results[0]:.2f}, Precision={results[1]:.2f}, Recall={results[2]:.2f}, F1-Score={results[3]:.2f}, AUC={results[4]:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-02341a52ad39>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_train, X_test, y_train, y_test, epsilon)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mX_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'parameters'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKKpfTJYBAyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "quixGYYTBAr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3lYE1e_BAoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4MrSU1EBAl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NAVJ9zm1BAji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DeWEyJ74BAhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWI4hK0zBAej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.decomposition import PCA, FactorAnalysis\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Real\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# Separating the features and target variable\n",
        "X = data.drop(columns=['is_patient'])\n",
        "y = data['is_patient']\n",
        "\n",
        "# Count the number of positive and negative cases\n",
        "num_positive_cases = (y == 1).sum()\n",
        "num_negative_cases = (y == 2).sum()\n",
        "\n",
        "# Calculate the desired number of samples for each class\n",
        "desired_num_samples = max(num_positive_cases, num_negative_cases)\n",
        "\n",
        "# Resample the data to balance the classes\n",
        "ros = RandomOverSampler(sampling_strategy={1: desired_num_samples, 2: desired_num_samples})\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Change class label 2 to 0 in y_resampled\n",
        "y_resampled[y_resampled == 2] = 0\n",
        "\n",
        "# Count the number of positive and negative cases after resampling\n",
        "num_positive_cases_resampled = (y_resampled == 1).sum()\n",
        "num_negative_cases_resampled = (y_resampled == 0).sum()\n",
        "\n",
        "# Creating a new DataFrame with the resampled data\n",
        "resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_data['is_patient'] = y_resampled\n",
        "\n",
        "# Impute missing values with the median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(resampled_data), columns=resampled_data.columns)\n",
        "\n",
        "# Detect and handle outliers\n",
        "z_scores = np.abs((data_imputed - data_imputed.mean()) / data_imputed.std())\n",
        "outliers = (z_scores > 3)\n",
        "for column in data_imputed.columns:\n",
        "    median = data_imputed[column].median()\n",
        "    data_imputed.loc[outliers[column], column] = median\n",
        "\n",
        "data_no_outliers = data_imputed\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(data_no_outliers), columns=data_no_outliers.columns)\n",
        "\n",
        "# Perform feature extraction using Bayesian optimization and L1/L2 regularization\n",
        "def objective(params):\n",
        "    selected_features, C, penalty = params[:-2], params[-2], params[-1]\n",
        "    selected_features = np.array(selected_features, dtype=bool)\n",
        "    X_selected = data_scaled.iloc[:, selected_features]\n",
        "\n",
        "    penalty = 'l1' if penalty < 0.5 else 'l2'\n",
        "    model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=42)\n",
        "    scores = cross_val_score(model, X_selected, y_resampled, cv=5, scoring='accuracy')\n",
        "    return -np.mean(scores)\n",
        "\n",
        "# Define the search space for Bayesian optimization\n",
        "n_features = data_scaled.shape[1]\n",
        "search_space = [Integer(0, 1) for _ in range(n_features)] + [Real(1e-6, 1e+6, prior='log-uniform'), Real(0, 1)]\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "result = gp_minimize(\n",
        "    func=objective,\n",
        "    dimensions=search_space,\n",
        "    n_calls=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Get the best features and parameters\n",
        "best_features = np.array(result.x[:-2], dtype=bool)\n",
        "best_C = result.x[-2]\n",
        "best_penalty = 'l1' if result.x[-1] < 0.5 else 'l2'\n",
        "selected_features_indices = np.arange(n_features)[best_features]\n",
        "selected_features_names = [f'feature_{i}' for i in selected_features_indices]\n",
        "\n",
        "print(\"Selected Features after Bayesian Optimization and Regularization:\")\n",
        "for feature_name in selected_features_names:\n",
        "    print(feature_name)\n",
        "print(f\"Best C: {best_C}\")\n",
        "print(f\"Best Penalty: {best_penalty}\")\n",
        "\n",
        "# Select the best features\n",
        "X_selected = data_scaled.iloc[:, selected_features_indices]\n",
        "\n",
        "# Apply Factor Analysis (FA)\n",
        "fa = FactorAnalysis(n_components=10)  # Adjust as needed\n",
        "X_fa = fa.fit_transform(X_selected)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_fa, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "log_reg = LogisticRegression(C=best_C, penalty=best_penalty, solver='liblinear', random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else np.zeros_like(y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba)\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Train-test split evaluation\n",
        "models = {\n",
        "    \"Logistic Regression\": log_reg,\n",
        "    \"K-Nearest Neighbors\": knn,\n",
        "    \"Random Forest\": rf,\n",
        "    \"SVM\": svm,\n",
        "    \"MLP\": mlp\n",
        "}\n",
        "\n",
        "train_test_results = {}\n",
        "for name, model in models.items():\n",
        "    results = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
        "    train_test_results[name] = results\n",
        "\n",
        "# 10-fold cross-validation evaluation\n",
        "def cross_val_evaluate_model(model, X, y):\n",
        "    accuracy = cross_val_score(model, X, y, cv=10, scoring='accuracy').mean()\n",
        "    precision = cross_val_score(model, X, y, cv=10, scoring='precision').mean()\n",
        "    recall = cross_val_score(model, X, y, cv=10, scoring='recall').mean()\n",
        "    f1 = cross_val_score(model, X, y, cv=10, scoring='f1').mean()\n",
        "    auc = cross_val_score(model, X, y, cv=10, scoring='roc_auc').mean()\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "cross_val_results = {}\n",
        "for name, model in models.items():\n",
        "    results = cross_val_evaluate_model(model, X_fa, y_resampled)\n",
        "    cross_val_results[name] = results\n",
        "\n",
        "# Ensemble classifier (example with majority voting)\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('log_reg', log_reg),\n",
        "    ('knn', knn),\n",
        "    ('rf', rf),\n",
        "    ('svm', svm),\n",
        "    ('mlp', mlp)\n",
        "], voting='soft')\n",
        "\n",
        "# Evaluate ensemble classifier\n",
        "ensemble_results = evaluate_model(ensemble, X_train, X_test, y_train, y_test)\n",
        "ensemble_cross_val_results = cross_val_evaluate_model(ensemble, X_fa, y_resampled)\n",
        "\n",
        "# Print results\n",
        "print(\"Train-Test Split Results\")\n",
        "for name, results in train_test_results.items():\n",
        "    print(f\"{name}: Accuracy={results[0]:.2f}, Precision={results[1]:.2f}, Recall={results[2]:.2f}, F1-Score={results[3]:.2f}, AUC={results[4]:.2f}\")\n",
        "\n",
        "print(\"\\n10-Fold Cross-Validation Results\")\n",
        "for name, results in cross_val_results.items():\n",
        "    print(f\"{name}: Accuracy={results[0]:.2f}, Precision={results[1]:.2f}, Recall={results[2]:.2f}, F1-Score={results[3]:.2f}, AUC={results[4]:.2f}\")\n",
        "\n",
        "print(\"\\nEnsemble Classifier Results (Train-Test Split):\")\n",
        "print(f\"Accuracy={ensemble_results[0]:.2f}, Precision={ensemble_results[1]:.2f}, Recall={ensemble_results[2]:.2f}, F1-Score={ensemble_results[3]:.2f}, AUC={ensemble_results[4]:.2f}\")\n",
        "\n",
        "print(\"\\nEnsemble Classifier Results (10-Fold Cross-Validation):\")\n",
        "print(f\"Accuracy={ensemble_cross_val_results[0]:.2f}, Precision={ensemble_cross_val_results[1]:.2f}, Recall\")\n"
      ],
      "metadata": {
        "id": "htfdyVLa46A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b4c072-a2da-43e5-d470-b2ff9f82e99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features after Bayesian Optimization and Regularization:\n",
            "feature_1\n",
            "feature_2\n",
            "feature_4\n",
            "feature_5\n",
            "feature_6\n",
            "feature_7\n",
            "feature_8\n",
            "feature_9\n",
            "feature_10\n",
            "Best C: 382880.8868165905\n",
            "Best Penalty: l2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-Test Split Results\n",
            "Logistic Regression: Accuracy=0.83, Precision=0.82, Recall=0.79, F1-Score=0.80, AUC=0.91\n",
            "K-Nearest Neighbors: Accuracy=0.91, Precision=0.93, Recall=0.87, F1-Score=0.90, AUC=0.97\n",
            "Random Forest: Accuracy=0.94, Precision=0.92, Recall=0.95, F1-Score=0.93, AUC=0.98\n",
            "SVM: Accuracy=0.88, Precision=0.89, Recall=0.84, F1-Score=0.86, AUC=0.93\n",
            "MLP: Accuracy=0.83, Precision=0.79, Recall=0.85, F1-Score=0.82, AUC=0.95\n",
            "\n",
            "10-Fold Cross-Validation Results\n",
            "Logistic Regression: Accuracy=0.87, Precision=0.90, Recall=0.84, F1-Score=0.87, AUC=0.93\n",
            "K-Nearest Neighbors: Accuracy=0.88, Precision=0.91, Recall=0.86, F1-Score=0.88, AUC=0.96\n",
            "Random Forest: Accuracy=0.96, Precision=0.99, Recall=0.94, F1-Score=0.96, AUC=0.99\n",
            "SVM: Accuracy=0.89, Precision=0.92, Recall=0.86, F1-Score=0.89, AUC=0.93\n",
            "MLP: Accuracy=0.89, Precision=0.89, Recall=0.89, F1-Score=0.89, AUC=0.95\n",
            "\n",
            "Ensemble Classifier Results (Train-Test Split):\n",
            "Accuracy=0.89, Precision=0.87, Recall=0.88, F1-Score=0.87, AUC=0.97\n",
            "\n",
            "Ensemble Classifier Results (10-Fold Cross-Validation):\n",
            "Accuracy=0.91, Precision=0.94, Recall\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGOPT4KFBJ3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}